{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = {}\n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(),np.random.rand(6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['petness'] = (.3 * tfidf['cat'] +\\\n",
    "   .3 * tfidf['dog'] +\\\n",
    "    0 * tfidf['apple'] +\\\n",
    "    0 * tfidf['lion'] -\\\n",
    "    .2 * tfidf['NYC'] +\\\n",
    "     .2 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['animalness'] = (.1 * tfidf['cat'] +\\\n",
    "    .1 * tfidf['dog'] -\\\n",
    "    .1 * tfidf['apple'] +\\\n",
    "    .5 * tfidf['lion'] +\\\n",
    "    .1 * tfidf['NYC'] -\\\n",
    "    1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['cityness'] = ( 0 * tfidf['cat'] -\\\n",
    "    .1 * tfidf['dog'] +\\\n",
    "    .2 * tfidf['apple'] -\\\n",
    "    .1 * tfidf['lion'] +\\\n",
    "    .5 * tfidf['NYC'] +\\\n",
    "    .1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = {}\n",
    "word_vector['cat'] = .3*topic['petness'] +.1*topic['animalness'] +0*topic['cityness']\n",
    "word_vector['dog'] = .3*topic['petness'] +.1*topic['animalness'] -.1*topic['cityness']\n",
    "word_vector['apple']= 0*topic['petness'] -.1*topic['animalness'] +.2*topic['cityness']\n",
    "word_vector['lion'] = 0*topic['petness'] +.5*topic['animalness'] -.1*topic['cityness']\n",
    "word_vector['NYC'] = -.2*topic['petness'] +.1*topic['animalness'] +.5*topic['cityness']\n",
    "word_vector['love'] = .2*topic['petness'] -.1*topic['animalness'] +.1*topic['cityness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "INFO:nlpia.loaders:No BIGDATA index found in C:\\Anaconda\\python\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy C:\\Anaconda\\python\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to C:\\Anaconda\\python\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Anaconda\\\\python\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Anaconda\\\\python\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Anaconda\\\\python\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'nrows': None, 'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "pd.options.display.width = 120\n",
    "sms = get_data('sms-spam')\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms['spam'] = sms.spam.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spam                                               text\n",
       "sms0      0  Go until jurong point, crazy.. Available only ...\n",
       "sms1      0                      Ok lar... Joking wif u oni...\n",
       "sms2!     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "sms3      0  U dun say so early hor... U c already then say...\n",
       "sms4      0  Nah I don't think he goes to usf, he lives aro...\n",
       "sms5!     1  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=sms.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sms.spam.astype(bool).values\n",
    "spam_centroid = tfidf_docs[mask].mean(axis=0)\n",
    "ham_centroid = tfidf_docs[~mask].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamminess_score = tfidf_docs.dot(spam_centroid -ham_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>lda_predict</th>\n",
       "      <th>lda_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spam  lda_predict  lda_score\n",
       "sms0      0            0       0.23\n",
       "sms1      0            0       0.18\n",
       "sms2!     1            1       0.72\n",
       "sms3      0            0       0.18\n",
       "sms4      0            0       0.29\n",
       "sms5!     1            1       0.55"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sms['lda_score'] = MinMaxScaler().fit_transform(spamminess_score.reshape(-1,1))\n",
    "sms['lda_predict'] = (sms.lda_score > .5).astype(int)\n",
    "sms['spam lda_predict lda_score'.split()].round(2).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分類是不是垃圾郵件，先算出垃圾郵件與非垃圾郵件各自的質心\\n   將兩個質心相減算出距離，以0.5為標準(主要是判斷算出的距離中垃圾郵件與非垃圾郵件所佔的比例)\\n   (sms.spam - sms.lda_predict).abs計算出錯誤的預測數\\n   再除以長度算出錯誤率，1-錯誤率極為正確率'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''分類是不是垃圾郵件，先算出垃圾郵件與非垃圾郵件各自的質心\n",
    "   將兩個質心相減算出距離，以0.5為標準(主要是判斷算出的距離中垃圾郵件與非垃圾郵件所佔的比例)\n",
    "   (sms.spam - sms.lda_predict).abs計算出錯誤的預測數\n",
    "   再除以長度算出錯誤率，1-錯誤率極為正確率'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\python\\lib\\site-packages\\pugnlp\\stats.py:504: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  self.__setattr__('_hist_labels', self.sum().astype(int))\n",
      "C:\\Anaconda\\python\\lib\\site-packages\\pugnlp\\stats.py:510: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  setattr(self, '_hist_classes', self.T.sum())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>lda_predict</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4135</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "lda_predict     0    1\n",
       "spam                  \n",
       "0            4135   64\n",
       "1              45  593"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pugnlp.stats import Confusion\n",
    "Confusion(sms['spam lda_predict'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cats_and_dogs_sorted.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    raw_data = []\n",
    "    for line in f:\n",
    "        raw_data.append(line.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cat  dog  apple  lion  nyc  love\n",
      "top0 -0.6 -0.4    0.5  -0.3  0.4  -0.1\n",
      "top1 -0.1 -0.3   -0.4  -0.1  0.1   0.8\n",
      "top2 -0.3  0.8   -0.1  -0.5  0.0   0.1\n"
     ]
    }
   ],
   "source": [
    "from nlpia.data.loaders import get_data\n",
    "from nltk.tokenize import casual_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.stem import PorterStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from nlpia.constants import DATA_PATH\n",
    "\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "NUM_WORDS = 6\n",
    "NUM_DOCS = NUM_PRETTY = 16\n",
    "SAVE_SORTED_CORPUS = ''  # 'cats_and_dogs_sorted.txt'\n",
    "# import nltk\n",
    "# nltk.download('wordnet')  # noqa\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# STOPWORDS = 'a an and or the do are with from for of on in by if at to into them'.split()\n",
    "# STOPWORDS += 'to at it its it\\'s that than our you your - -- \" \\' ? , . !'.split()\n",
    "STOPWORDS = []\n",
    "\n",
    "# SYNONYMS = dict(zip(\n",
    "#     'wolv people person women woman man human he  we  her she him his hers'.split(),\n",
    "#     'wolf her    her    her   her   her her   her her her her her her her'.split()))\n",
    "# SYNONYMS.update(dict(zip(\n",
    "#     'ate pat smarter have had isn\\'t hasn\\'t no  got get become been was were wa be sat seat sit'.split(),\n",
    "#     'eat pet smart   has  has not    not     not has has is     is   is  is   is is sit sit  sit'.split())))\n",
    "# SYNONYMS.update(dict(zip(\n",
    "#     'i me my mine our ours catbird bird birds birder tortoise turtle turtles turtle\\'s don\\'t'.split(),\n",
    "#     'i i  i  i    i   i    bird    bird birds bird   turtle   turtle turtle  turtle    not'.split())))\n",
    "SYNONYMS = {}\n",
    "\n",
    "stemmer = None  # PorterStemmer()\n",
    "\n",
    "pd.options.display.width = 110\n",
    "pd.options.display.max_columns = 14\n",
    "pd.options.display.max_colwidth = 32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_corpus_words(corpus, stemmer=stemmer, synonyms=SYNONYMS, stopwords=STOPWORDS):\n",
    "    docs = [doc.lower() for doc in corpus]\n",
    "    docs = [casual_tokenize(doc) for doc in docs]\n",
    "    docs = [[synonyms.get(w, w) for w in words if w not in stopwords] for words in docs]\n",
    "    if stemmer:\n",
    "        docs = [[stemmer.stem(w) for w in words if w not in stopwords] for words in docs]\n",
    "    docs = [[synonyms.get(w, w) for w in words if w not in stopwords] for words in docs]\n",
    "    docs = [' '.join(w for w in words if w not in stopwords) for words in docs]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def tokenize(text, vocabulary, synonyms=SYNONYMS, stopwords=STOPWORDS):\n",
    "    doc = normalize_corpus_words([text.lower()], synonyms=synonyms, stopwords=stopwords)[0]\n",
    "    stems = [w for w in doc.split() if w in vocabulary]\n",
    "    return stems\n",
    "\n",
    "\n",
    "fun_words = vocabulary = 'cat dog apple lion nyc love big small'\n",
    "fun_stems = normalize_corpus_words([fun_words])[0].split()[:NUM_WORDS]\n",
    "fun_words = fun_words.split()\n",
    "\n",
    "\n",
    "if SAVE_SORTED_CORPUS:\n",
    "    tfidfer = TfidfVectorizer(min_df=2, max_df=.6, stop_words=None, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "\n",
    "    corpus = get_data('cats_and_dogs')[:NUM_DOCS]\n",
    "    docs = normalize_corpus_words(corpus, stemmer=None)\n",
    "    tfidf_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "    id_words = [(i, w) for (w, i) in tfidfer.vocabulary_.items()]\n",
    "    tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "\n",
    "\n",
    "    word_tfidf_dense = pd.DataFrame(tfidfer.transform(fun_stems).todense())\n",
    "    word_tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "    word_tfidf_dense.index = fun_stems\n",
    "    \"\"\"\n",
    "    >>> word_tfidf_dense[fun_stems]\n",
    "          cat  dog  anim  pet  citi  appl  nyc  car  bike  hat\n",
    "    cat   1.0  0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0  0.0\n",
    "    dog   0.0  1.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0  0.0\n",
    "    anim  0.0  0.0   1.0  0.0   0.0   0.0  0.0  0.0   0.0  0.0\n",
    "    pet   0.0  0.0   0.0  1.0   0.0   0.0  0.0  0.0   0.0  0.0\n",
    "    citi  0.0  0.0   0.0  0.0   1.0   0.0  0.0  0.0   0.0  0.0\n",
    "    appl  0.0  0.0   0.0  0.0   0.0   1.0  0.0  0.0   0.0  0.0\n",
    "    nyc   0.0  0.0   0.0  0.0   0.0   0.0  1.0  0.0   0.0  0.0\n",
    "    car   0.0  0.0   0.0  0.0   0.0   0.0  0.0  1.0   0.0  0.0\n",
    "    bike  0.0  0.0   0.0  0.0   0.0   0.0  0.0  0.0   1.0  0.0\n",
    "    hat   0.0  0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0  1.0\n",
    "    \"\"\"\n",
    "\n",
    "    tfidfer.use_idf = False\n",
    "    tfidfer.norm = None\n",
    "    bow_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "    bow_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "    bow_dense = bow_dense.astype(int)\n",
    "    tfidfer.use_idf = True\n",
    "    tfidfer.norm = 'l2'\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    >>> tfidf_dense.shape\n",
    "    (200, 170)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    bow_pretty = bow_dense.copy()\n",
    "    bow_pretty = bow_pretty[fun_stems]\n",
    "    bow_pretty['text'] = corpus\n",
    "\n",
    "    tfidf_pretty = tfidf_dense.copy()\n",
    "    tfidf_pretty = tfidf_pretty[fun_stems]\n",
    "    tfidf_pretty['diversity'] = tfidf_pretty[fun_stems].T.sum().values\n",
    "    tfidf_pretty['text'] = corpus\n",
    "    # tfidf_pretty['diversity'] = [(row.diversity or 0) / ((float(row.iloc[i % (len(row) - 2)] or 1) ** 2))\n",
    "    #                              for i, row in tfidf_pretty.iterrows()]\n",
    "    tfidf_pretty = tfidf_pretty.sort_values('diversity', ascending=False).round(2)\n",
    "    with open(os.path.join(DATA_PATH, SAVE_SORTED_CORPUS), 'w') as fout:\n",
    "        fout.write('\\n'.join(list(tfidf_pretty.text.values)))\n",
    "\n",
    "    for col in fun_stems:\n",
    "        bow_pretty.loc[bow_pretty[col] == 0, col] = ''\n",
    "    # print(bow_pretty.head())\n",
    "\n",
    "\n",
    "# do it all over again on a tiny portion of the corpus and vocabulary\n",
    "import os\n",
    "domin=os.path.abspath(r'C:\\Users\\ChuangYu\\Desktop\\NLP')\n",
    "info = os.path.join(domin,'cats_and_dogs_sorted.txt') #將路徑與檔名結合起來就是每個檔案的完整路徑\n",
    "info = open(info,'r',encoding=\"utf-8\") #讀取檔案內容\n",
    "\n",
    "corpus=info.read().split(\"\\n\")\n",
    "corpus=corpus[:NUM_PRETTY]\n",
    "docs = normalize_corpus_words(corpus)\n",
    "tfidfer = TfidfVectorizer(min_df=1, max_df=.99, stop_words=None, token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                          vocabulary=fun_stems)\n",
    "tfidf_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "id_words = [(i, w) for (w, i) in tfidfer.vocabulary_.items()]\n",
    "tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "tfidfer.use_idf = False\n",
    "tfidfer.norm = None\n",
    "bow_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "bow_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "bow_dense = bow_dense.astype(int)\n",
    "tfidfer.use_idf = True\n",
    "tfidfer.norm = 'l2'\n",
    "bow_pretty = bow_dense.copy()\n",
    "bow_pretty = bow_pretty[fun_stems]\n",
    "bow_pretty['text'] = corpus\n",
    "for col in fun_stems:\n",
    "    bow_pretty.loc[bow_pretty[col] == 0, col] = ''\n",
    "# print(bow_pretty)\n",
    "word_tfidf_dense = pd.DataFrame(tfidfer.transform(fun_stems).todense())\n",
    "word_tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "word_tfidf_dense.index = fun_stems\n",
    "\n",
    "tfidf_pretty = tfidf_dense.copy()\n",
    "tfidf_pretty = tfidf_pretty[fun_stems]\n",
    "tfidf_pretty = tfidf_pretty.round(2)\n",
    "for col in fun_stems:\n",
    "    tfidf_pretty.loc[tfidf_pretty[col] == 0, col] = ''\n",
    "# tfidf_pretty[:text]tfidf_pretty.text.str[:16]\n",
    "# print(tfidf_pretty)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ">>> pd.options.display.width = 150\n",
    ">>> pd.options.display.max_columns = 16\n",
    ">>> tfidf_pretty = tfidf_dense.copy()\n",
    ">>> tfidf_pretty = tfidf_pretty[['bike', 'cat', 'car', 'chase', 'dog', 'hat', 'i']].head(10).round(1)\n",
    ">>> tfidf_pretty[tfidf_pretty == 0] = ''\n",
    ">>> tfidf_pretty['...'] = ''\n",
    ">>> tfidf_pretty['text'] = corpus[:10]\n",
    ">>> tfidf_pretty\n",
    "   dog  cat bear  pet  hat bike chase bark meow ...                                         text\n",
    "0       0.4            0.9                                                    The Cat in the Hat\n",
    "1       0.5                                                                   The cat ate a rat.\n",
    "2       0.6                       0.8                           The cat chased my laser pointer.\n",
    "3  0.3                      0.6   0.4  0.6               A dog chased my bike and barked loudly.\n",
    "4  0.4                                 0.6                           I ran from the barking dog.\n",
    "5  0.4                            0.5  0.6                        A dog chased the car, barking.\n",
    "6       0.5                                 0.9                                   The Cat's Meow\n",
    "7       0.3       0.4                       0.5      The cat meowed so I pet it until it purred.\n",
    "8       0.5                                 0.9                 A cat meowed on the hot tin roof\n",
    "9  0.5  0.4                                                      Cats and dogs playing together.\n",
    "\"\"\"\n",
    "\n",
    "tfidf_zeros = tfidf_dense.T.sum()[tfidf_dense.T.sum() == 0]\n",
    "# print(tfidf_zeros)\n",
    "\"\"\"\n",
    ">>> tfidf_zeros = tfidf_dense.T.sum()[tfidf_dense.T.sum() == 0]\n",
    ">>> tfidf_zeros\n",
    "199    0.0\n",
    "\"\"\"\n",
    "\n",
    "[corpus[i] for i in tfidf_zeros.index]\n",
    "\"\"\"\n",
    ">>> [corpus[i] for i in tfidf_zeros.index]\n",
    "[]\n",
    "\n",
    "# ['I flew a kite.', 'He froze.']\n",
    "\"\"\"\n",
    "\n",
    "pcaer = PCA(n_components=NUM_TOPICS)\n",
    "\n",
    "doc_topic_vectors = pd.DataFrame(pcaer.fit_transform(tfidf_dense.values), columns=['top{}'.format(i) for i in range(NUM_TOPICS)])\n",
    "doc_topic_vectors['text'] = corpus\n",
    "pd.options.display.max_colwidth = 55\n",
    "# doc_topic_vectors.round(1)\n",
    "\"\"\"\n",
    ">>> doc_topic_vectors.round(1)\n",
    "    topic_A  topic_B  topic_C  topic_D                                                             text\n",
    "0      -0.2      0.1      0.5     -0.2  Animals don't drive cars, but my pet dog likes to stick his ...\n",
    "1       0.1      0.1     -0.0     -0.3              The Cat in the Hat is not about an animal or a hat.\n",
    "2       0.1     -0.2      0.5     -0.3                   Cats don't like riding into the city in a car.\n",
    "3      -0.2     -0.5      0.0      0.3                      Dogs love to chase cars, trucks, and bikes.\n",
    "4      -0.2     -0.3      0.2      0.1        Wild cats chase bikes and runners but not cars or trucks.\n",
    "5      -0.1     -0.0      0.6     -0.2              Animals, including pets, don't like riding in cars.\n",
    "6       0.6      0.0     -0.1      0.0                                 NYC is a city that never sleeps.\n",
    "7       0.7      0.1      0.0      0.2                                  Come to NYC. See the Big Apple!\n",
    "8       0.8      0.1      0.0      0.2                                   NYC is known as the Big Apple.\n",
    "9       0.7      0.1     -0.0      0.1  NYC is the only city where you can hardly find a typical Ame...\n",
    "10     -0.2     -0.1     -0.5     -0.3                                         It rained cats and dogs.\n",
    "11     -0.4      0.5     -0.0      0.4                                               I love my pet cat.\n",
    "12      0.3      0.1     -0.1      0.4                                      I love New York City (NYC).\n",
    "13     -0.2      0.3      0.2     -0.2                                      He pet the dog on the head.\n",
    "14     -0.2     -0.5      0.2      0.0                                         Dogs like to chase cars.\n",
    "15     -0.2     -0.1     -0.3     -0.1                          The cat steered clear of the dog house.\n",
    "16     -0.1     -0.4      0.3      0.1                                         The car had a bike rack.\n",
    "\"\"\"\n",
    "\n",
    "word_topic_vectors = pd.DataFrame(pcaer.transform(word_tfidf_dense.values),\n",
    "                                  columns=['top{}'.format(i) for i in range(NUM_TOPICS)])\n",
    "word_topic_vectors.index = fun_stems\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ">>> tfidf_similarity = []\n",
    "... topic_similarity = []\n",
    "... for i in range(10):\n",
    "...     topic_similarity.append((doc_topic_vectors.iloc[i] * doc_topic_vectors.iloc[i+1]).sum())\n",
    "...     tfidf_similarity.append((tfidf_dense.iloc[i] * tfidf_dense.iloc[i+1]).sum())\n",
    ">>> tfidf_pretty['tfidf_similar'] = tfidf_similarity\n",
    ">>> tfidf_pretty['topic_similar'] = topic_similarity\n",
    ">>> tfidf_pretty\n",
    "  bike  cat  car chase  dog  hat    i ...                                     text  tfidf_similar  topic_similar\n",
    "0       0.4                  0.9                                The Cat in the Hat       0.217066       0.208128\n",
    "1       0.5                                                     The cat ate a rat.       0.000000      -0.085262\n",
    "2  0.5                            0.6                         I rode my bike home.       0.000000      -0.075669\n",
    "3            0.5                                         The car is in the garage.       0.267063       0.045623\n",
    "4            0.5   0.5  0.4                               Dogs like to chase cars.       0.620088       0.214047\n",
    "5                       0.4                            The post man likes our dog.       0.316328       0.164364\n",
    "6                       0.3                  He refused to sleep in the dog house.       0.000000      -0.057364\n",
    "7       0.6        0.8                                     The cat chased a mouse.       0.000000      -0.004849\n",
    "8                       0.4       0.4           I was in the dog house last night.       0.488741       0.025706\n",
    "9       0.3             0.3                The cat steered clear of the dog house.       0.000000       0.011857\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tfidf_search(text, corpus=tfidf_dense, corpus_text=corpus):\n",
    "    \"\"\" search for the most relevant document \"\"\"\n",
    "    tokens = tokenize(text, vocabulary=corpus.columns)\n",
    "    tfidf_vector_query = np.array(tfidfer.transform([' '.join(tokens)]).todense())[0]\n",
    "    query_series = pd.Series(tfidf_vector_query, index=corpus.columns)\n",
    "\n",
    "    return corpus_text[query_series.dot(corpus.T).values.argmax()]\n",
    "\n",
    "\n",
    "def topic_search(text, corpus=doc_topic_vectors, pcaer=pcaer, corpus_text=corpus):\n",
    "    \"\"\" search for the most relevant document \"\"\"\n",
    "    tokens = tokenize(text, vocabulary=corpus.columns)\n",
    "    tfidf_vector_query = np.array(tfidfer.transform([' '.join(tokens)]).todense())[0]\n",
    "    topic_vector_query = pcaer.transform([tfidf_vector_query])\n",
    "    query_series = pd.Series(topic_vector_query, index=corpus.columns)\n",
    "    return corpus_text[query_series.dot(corpus.T).values.argmax()]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tfidf_search('Hello world, do you have a cat?')\n",
    "\n",
    "topic_search('Hello world, do you have a cat?')\n",
    "# 'Do you have a pet?'\n",
    "\n",
    "search('The quick brown fox jumped over the lazy dog')\n",
    "# 'The dog sat on the floor.'\n",
    "\n",
    "search('A dog barked at my car incessantly.')\n",
    "# 'A dog chased the car, barking.'\n",
    "tokenize('A dog barked at my car incessantly.')\n",
    "# ['dog', 'bark', 'at', 'car', 'incessantli']\n",
    "\n",
    "search('A Rotweiller barked at my car incessantly.')\n",
    "# 'The cat hated getting in the car.'\n",
    "tokenize('A Rotweiller barked at my car incessantly.')\n",
    "# ['rotweil', 'bark', 'at', 'car', 'incessantli']\n",
    "\n",
    "list(df.columns)\n",
    "# ['ate', 'can', 'car', 'cat', 'chase', 'cute', 'die', 'dog', 'ferret', 'flower', 'hair',\n",
    "# 'hat', 'have', 'it', 'kitten', 'pet', 'ran',\n",
    "#   'squirrel', 'struck', 'took', 'tree', 'trick', 'turtl', 'up', 'vet', 'water'],\n",
    "\"\"\"\n",
    "U, Sigma, VT = np.linalg.svd(tfidf_dense.T)  # <1> Transpose the doc-word tfidf matrix, because SVD works on column vectors\n",
    "S = Sigma.copy()\n",
    "S[4:] = 0\n",
    "doc_labels = ['doc{}'.format(i) for i in range(len(tfidf_dense))]\n",
    "U_df = pd.DataFrame(U, index=fun_stems, columns=fun_stems)\n",
    "VT_df = pd.DataFrame(VT, index=doc_labels, columns=doc_labels)\n",
    "ndim = 2\n",
    "truncated_tfidf = U[:, :ndim].dot(np.diag(Sigma)[:ndim, :ndim]).dot(VT.T[:, :ndim].T)\n",
    "\"\"\"\n",
    "The left singular vectors tell you how to \"rotate\" the TF-IDF vectors into the topic space, equivalent to creating topics\n",
    "\n",
    ">>> U_df\n",
    "       cat   dog  appl   nyc   car  bike   hat\n",
    "cat  -0.53  0.01 -0.50  0.31 -0.49 -0.00 -0.36\n",
    "dog  -0.60  0.25  0.19  0.43  0.56 -0.00  0.21\n",
    "appl -0.16 -0.63  0.17 -0.12  0.37 -0.00 -0.63\n",
    "nyc  -0.25 -0.69  0.06  0.04 -0.24  0.00  0.63\n",
    "car  -0.35  0.17  0.32 -0.45 -0.21  0.71 -0.03\n",
    "bike -0.35  0.17  0.32 -0.45 -0.21 -0.71 -0.03\n",
    "hat  -0.17  0.00 -0.69 -0.55  0.40  0.00  0.19\n",
    "\n",
    ">>> VT_df.round(2)\n",
    "        doc0  doc1  doc2  doc3  doc4  doc5  doc6  doc7  doc8  doc9  doc10  doc11\n",
    "doc0  -0.37 -0.34 -0.16 -0.22 -0.33 -0.33 -0.27 -0.15 -0.40 -0.40  -0.15  -0.15\n",
    "doc1   0.19  0.12  0.00  0.01  0.16  0.16 -0.29 -0.51  0.11  0.11  -0.51  -0.51\n",
    "doc2   0.33  0.11 -0.55 -0.58  0.25  0.25 -0.19  0.11 -0.13 -0.13   0.11   0.11\n",
    "doc3  -0.31 -0.39 -0.39 -0.27 -0.07 -0.07  0.21 -0.06  0.48  0.48  -0.06  -0.06\n",
    "doc4   0.03 -0.59  0.28  0.08  0.24  0.24 -0.61  0.14  0.10  0.10   0.14   0.14\n",
    "doc5   0.00  0.00  0.00 -0.00  0.71 -0.71 -0.00 -0.00  0.00  0.00  -0.00  -0.00\n",
    "doc6   0.16 -0.51  0.17 -0.10  0.27  0.27  0.62 -0.11 -0.23 -0.23  -0.11  -0.11\n",
    "doc7  -0.04 -0.07 -0.36  0.41  0.09  0.09  0.00  0.67 -0.06 -0.06  -0.33  -0.33\n",
    "doc8  -0.54  0.19  0.07 -0.08  0.27  0.27  0.00 -0.00  0.45 -0.55  -0.00  -0.00\n",
    "doc9  -0.54  0.19  0.07 -0.08  0.27  0.27  0.00 -0.00 -0.55  0.45  -0.00  -0.00\n",
    "doc10 -0.04 -0.07 -0.36  0.41  0.09  0.09  0.00 -0.33 -0.06 -0.06   0.67  -0.33\n",
    "doc11 -0.04 -0.07 -0.36  0.41  0.09  0.09  0.00 -0.33 -0.06 -0.06  -0.33   0.67\n",
    "Try to reconstruct an approximate TFIDF, using only 2 topics (from 7 words):\n",
    "\n",
    ">>> tfidf_compressed = U[:,:2] @ (pd.np.diag(S)[:2,:] @ VT[:2,:])\n",
    ">>> tfidf_compressed.shape\n",
    "\n",
    "array([[ 0.12191697,  0.01013273,  0.04009995, ...,  0.06057937,\n",
    "         0.07675374, -0.00521042],\n",
    "       [ 0.0352883 ,  0.07193914,  0.00248612, ...,  0.00928599,\n",
    "         0.02413519,  0.02511732],\n",
    "       [ 0.0886943 ,  0.0689894 ,  0.06191171, ...,  0.08041404,\n",
    "        -0.0090325 ,  0.01368156],\n",
    "       ...,\n",
    "       [-0.00116523,  0.00980797,  0.00397578, ..., -0.00540478,\n",
    "         0.04008382,  0.01717131],\n",
    "       [ 0.08695312,  0.01880789,  0.04888827, ...,  0.0604496 ,\n",
    "         0.04363558,  0.00425347],\n",
    "       [ 0.06353676, -0.01267888,  0.0766847 , ...,  0.07791765,\n",
    "         0.01870914,  0.00097211]])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(word_topic_vectors.T.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>apple</th>\n",
       "      <th>lion</th>\n",
       "      <th>nyc</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>top0</th>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top1</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top2</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat  dog  apple  lion  nyc  love\n",
       "top0 -0.6 -0.4    0.5  -0.3  0.4  -0.1\n",
       "top1 -0.1 -0.3   -0.4  -0.1  0.1   0.8\n",
       "top2 -0.3  0.8   -0.1  -0.5  0.0   0.1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic_vectors.T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -f C:\\Users\\ChuangYu\\AppData\\Roaming\\jupyter\\runtime\\kernel-e82623fd-15f2-4d2b-897c-761ef2213ef1.json  \\\n",
      "0                                                                                                          \n",
      "1                                                                                                          \n",
      "2                                                                                                          \n",
      "3                                                                                                          \n",
      "4                                                                                                          \n",
      "5                                                                                                          \n",
      "6                                                                                                          \n",
      "7                                                                                                          \n",
      "8                                                                                                          \n",
      "9                                                                                                          \n",
      "10                                                                                                         \n",
      "\n",
      "                                               text  \n",
      "0                             NYC is the Big Apple.  \n",
      "1                    NYC is known as the Big Apple.  \n",
      "2                                       I love NYC!  \n",
      "3       I wore a hat to the Big Apple party in NYC.  \n",
      "4                   Come to NYC. See the Big Apple!  \n",
      "5                Manhattan is called the Big Apple.  \n",
      "6           New York is a big city for a small cat.  \n",
      "7   The lion, a big cat, is the king of the jungle.  \n",
      "8                                I love my pet cat.  \n",
      "9                       I love New York City (NYC).  \n",
      "10                          Your dog chased my cat.  \n",
      "BOW accuracy after multiplying Truncated SVD back together\n",
      "[1.0, 1.0]\n",
      "TF-IDF accuracy after multiplying Truncated SVD back together\n",
      "[1.0, 1.0]\n",
      "Would you like to see the accuracy comparison plot? [y]y\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Uses np.linalg.svd directly to illustrate LSA on a small corpus. \n",
    "\n",
    "Examples from the SVD section in Chapter 4 of NLPIA\n",
    "\n",
    "SEE ALSO: ch04_stanford_lsa.py\n",
    "\n",
    ">>> from nlpia.book.examples.ch04_catdog_lsa_sorted import lsa_models, prettify_tdm\n",
    ">>> bow_svd, tfidf_svd = lsa_models()  # <1>\n",
    ">>> prettify_tdm(**bow_svd)\n",
    "   cat dog apple lion nyc love                                             text\n",
    "0              1        1                                 NYC is the Big Apple.\n",
    "1              1        1                        NYC is known as the Big Apple.\n",
    "2                       1    1                                      I love NYC!\n",
    "3              1        1           I wore a hat to the Big Apple party in NYC.\n",
    "4              1        1                       Come to NYC. See the Big Apple!\n",
    "5              1                             Manhattan is called the Big Apple.\n",
    "6    1                                  New York is a big city for a small cat.\n",
    "7    1              1           The lion, a big cat, is the king of the jungle.\n",
    "8    1                       1                               I love my pet cat.\n",
    "9                       1    1                      I love New York City (NYC).\n",
    "10   1   1                                              Your dog chased my cat.\n",
    ">>> tdm = bow_svd['tdm']\n",
    ">>> tdm\n",
    "\n",
    "       0   1   2   3   4   5   6   7   8   9   10\n",
    "cat     0   0   0   0   0   0   1   1   1   0   1\n",
    "dog     0   0   0   0   0   0   0   0   0   0   1\n",
    "apple   1   1   0   1   1   1   0   0   0   0   0\n",
    "lion    0   0   0   0   0   0   0   1   0   0   0\n",
    "nyc     1   1   1   1   1   0   0   0   0   1   0\n",
    "love    0   0   1   0   0   0   0   0   1   1   0\n",
    ">>> import numpy as np\n",
    ">>> U, s, Vt = np.linalg.svd(tdm)  # <1>\n",
    " \n",
    ">>> import pandas as pd\n",
    ">>> pd.DataFrame(U, index=tdm.index).round(2)\n",
    "          0     1     2     3     4     5\n",
    "cat   -0.04  0.83 -0.38 -0.00  0.11 -0.38\n",
    "dog   -0.00  0.21 -0.18 -0.71 -0.39  0.52\n",
    "apple -0.62 -0.21 -0.51  0.00  0.49  0.27\n",
    "lion  -0.00  0.21 -0.18  0.71 -0.39  0.52\n",
    "nyc   -0.75 -0.00  0.24 -0.00 -0.52 -0.32\n",
    "love  -0.22  0.42  0.69  0.00  0.41  0.37\n",
    "\n",
    ">>> s.round(1)\n",
    "array([3.1, 2.2, 1.8, 1. , 0.8, 0.5])\n",
    ">>> S = np.zeros((len(U), len(Vt)))\n",
    ">>> pd.np.fill_diagonal(S, s)\n",
    ">>> pd.DataFrame(S).round(1)\n",
    "    0    1    2    3    4    5    6    7    8    9    10\n",
    "0  3.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "1  0.0  2.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "2  0.0  0.0  1.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "4  0.0  0.0  0.0  0.0  0.8  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0\n",
    "\n",
    ">>> pd.DataFrame(Vt).round(2)\n",
    "      0     1     2     3     4     5     6     7     8     9     10\n",
    "0  -0.44 -0.44 -0.31 -0.44 -0.44 -0.20 -0.01 -0.01 -0.08 -0.31 -0.01\n",
    "1  -0.09 -0.09  0.19 -0.09 -0.09 -0.09  0.37  0.47  0.56  0.19  0.47\n",
    "2  -0.16 -0.16  0.52 -0.16 -0.16 -0.29 -0.22 -0.32  0.17  0.52 -0.32\n",
    "3   0.00 -0.00 -0.00  0.00  0.00  0.00 -0.00  0.71  0.00 -0.00 -0.71\n",
    "4  -0.04 -0.04 -0.14 -0.04 -0.04  0.58  0.13 -0.33  0.62 -0.14 -0.33\n",
    "5  -0.09 -0.09  0.10 -0.09 -0.09  0.51 -0.73  0.27 -0.01  0.10  0.27\n",
    "6  -0.57  0.21  0.11  0.33 -0.31  0.34  0.34 -0.00 -0.34  0.23  0.00\n",
    "7  -0.32  0.47  0.25 -0.63  0.41  0.07  0.07  0.00 -0.07 -0.18  0.00\n",
    "8  -0.50  0.29 -0.20  0.41  0.16 -0.37 -0.37 -0.00  0.37 -0.17  0.00\n",
    "9  -0.15 -0.15 -0.59 -0.15  0.42  0.04  0.04 -0.00 -0.04  0.63 -0.00\n",
    "10 -0.26 -0.62  0.33  0.24  0.54  0.09  0.09 -0.00 -0.09 -0.23 -0.00\n",
    "\n",
    ">>> tdm = bow_svd['tdm']\n",
    ">>> U, s, Vt = np.linalg.svd(tdm)\n",
    ">>> S = np.zeros((len(U), len(Vt)))\n",
    ">>> np.fill_diagonal(S, s)\n",
    ">>> err = [0]\n",
    ">>> for numdim in range(len(s), 0, -1):\n",
    "...     S[numdim - 1, numdim - 1] = 0\n",
    "...     reconstructed_tdm = U.dot(S).dot(Vt)\n",
    "...     err.append(np.sqrt(((reconstructed_tdm - tdm).values.flatten() ** 2).sum() \n",
    "...                / np.product(tdm.shape)))\n",
    ">>> np.array(err).round(2)\n",
    "array([0.  , 0.06, 0.12, 0.17, 0.28, 0.39, 0.55])\n",
    "\n",
    ">>> tdm = tfidf_svd['tdm']\n",
    ">>> U, s, Vt = np.linalg.svd(tdm)\n",
    ">>> S = np.zeros((len(U), len(Vt)))\n",
    ">>> np.fill_diagonal(S, s)\n",
    ">>> err2 = [0]\n",
    ">>> for numdim in range(len(s), 0, -1):\n",
    "...     S[numdim - 1, numdim - 1] = 0\n",
    "...     reconstructed_tdm = U.dot(S).dot(Vt)\n",
    "...     err2.append(np.sqrt(((reconstructed_tdm - tdm).values.flatten() ** 2).sum() \n",
    "...                / np.product(tdm.shape)))\n",
    ">>> np.array(err2).round(2)\n",
    "array([0.  , 0.07, 0.11, 0.15, 0.23, 0.3 , 0.41])\n",
    "\"\"\"\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # noqa\n",
    "import seaborn  # noqa\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nlpia.data.loaders import get_data\n",
    "\n",
    "pd.options.display.width = 120\n",
    "pd.options.display.max_columns = 16\n",
    "\n",
    "VOCABULARY = vocabulary='cat dog apple lion NYC love'.lower().split()  # 'cat dog apple lion NYC love big small bright'.lower().split()\n",
    "with open('cats_and_dogs_sorted.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    raw_data = []\n",
    "    for line in f:\n",
    "        raw_data.append(line.replace(\"\\n\", \"\"))\n",
    "DOCS = raw_data\n",
    "\n",
    "\n",
    "def docs_to_tdm(docs=DOCS, vocabulary=VOCABULARY, verbosity=0):\n",
    "    tfidfer = TfidfVectorizer(min_df=1, max_df=.99, stop_words=None, token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                              vocabulary=vocabulary)\n",
    "    tfidf_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "    id_words = [(i, w) for (w, i) in tfidfer.vocabulary_.items()]\n",
    "    tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "\n",
    "    tfidfer.use_idf = False\n",
    "    tfidfer.norm = None\n",
    "    bow_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "    bow_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "    bow_dense = bow_dense.astype(int)\n",
    "    tfidfer.use_idf = True\n",
    "    tfidfer.norm = 'l2'\n",
    "    if verbosity:\n",
    "        print(tfidf_dense.T)\n",
    "    return bow_dense.T, tfidf_dense.T, tfidfer\n",
    "\n",
    "\n",
    "def prettify_tdm(tdm=None, docs=[], vocabulary=[], **kwargs):\n",
    "    bow_pretty = tdm.T.copy()[vocabulary]\n",
    "    bow_pretty['text'] = docs\n",
    "    for col in vocabulary:\n",
    "        bow_pretty.loc[bow_pretty[col] == 0, col] = ''\n",
    "    return bow_pretty\n",
    "\n",
    "\n",
    "def accuracy_study(tdm=None, u=None, s=None, vt=None, verbosity=0, **kwargs):\n",
    "    \"\"\" Reconstruct the term-document matrix and measure error as SVD terms are truncated\n",
    "    \"\"\"\n",
    "    smat = np.zeros((len(u), len(vt)))\n",
    "    np.fill_diagonal(smat, s)\n",
    "    smat = pd.DataFrame(smat, columns=vt.index, index=u.index)\n",
    "    if verbosity:\n",
    "        print()\n",
    "        print('Sigma:')\n",
    "        print(smat.round(2))\n",
    "        print()\n",
    "        print('Sigma without zeroing any dim:')\n",
    "        print(np.diag(smat.round(2)))\n",
    "    tdm_prime = u.values.dot(smat.values).dot(vt.values)\n",
    "    if verbosity:\n",
    "        print()\n",
    "        print('Reconstructed Term-Document Matrix')\n",
    "        print(tdm_prime.round(2))\n",
    "\n",
    "    err = [np.sqrt(((tdm_prime - tdm).values.flatten() ** 2).sum() / np.product(tdm.shape))]\n",
    "    if verbosity:\n",
    "        print()\n",
    "        print('Error without reducing dimensions:')\n",
    "        print(err[-1])\n",
    "    # 2.3481474529927113e-15\n",
    "\n",
    "    smat2 = smat.copy()\n",
    "    for numdim in range(len(s) - 1, 0, -1):\n",
    "        smat2.iloc[numdim, numdim] = 0\n",
    "        if verbosity:\n",
    "            print('Sigma after zeroing out dim {}'.format(numdim))\n",
    "            print(np.diag(smat2.round(2)))\n",
    "            #           d0    d1   d2   d3   d4   d5\n",
    "            # ship    2.16  0.00  0.0  0.0  0.0  0.0\n",
    "            # boat    0.00  1.59  0.0  0.0  0.0  0.0\n",
    "            # ocean   0.00  0.00  0.0  0.0  0.0  0.0\n",
    "            # voyage  0.00  0.00  0.0  0.0  0.0  0.0\n",
    "            # trip    0.00  0.00  0.0  0.0  0.0  0.0\n",
    "\n",
    "        tdm_prime2 = u.values.dot(smat2.values).dot(vt.values)\n",
    "        err += [np.sqrt(((tdm_prime2 - tdm).values.flatten() ** 2).sum() / np.product(tdm.shape))]\n",
    "        if verbosity:\n",
    "            print('Error after zeroing out dim {}'.format(numdim))\n",
    "            print(err[-1])\n",
    "    return err\n",
    "\n",
    "\n",
    "def lsa(tdm, verbosity=0):\n",
    "    if verbosity:\n",
    "        print(tdm)\n",
    "        #         0   1   2   3   4   5   6   7   8   9   10\n",
    "        # cat     0   0   0   0   0   0   1   1   1   0   1\n",
    "        # dog     0   0   0   0   0   0   0   0   0   0   1\n",
    "        # apple   1   1   0   1   1   1   0   0   0   0   0\n",
    "        # lion    0   0   0   0   0   0   0   1   0   0   0\n",
    "        # love    0   0   1   0   0   0   0   0   1   1   0\n",
    "        # nyc     1   1   1   1   1   0   0   0   0   1   0\n",
    "\n",
    "    u, s, vt = np.linalg.svd(tdm)\n",
    "\n",
    "    u = pd.DataFrame(u, index=tdm.index)\n",
    "    if verbosity:\n",
    "        print('U')\n",
    "        print(u.round(2))\n",
    "        # U\n",
    "        #           0     1     2     3     4     5\n",
    "        # cat   -0.04  0.83 -0.38 -0.00  0.11  0.38\n",
    "        # dog   -0.00  0.21 -0.18 -0.71 -0.39 -0.52\n",
    "        # apple -0.62 -0.21 -0.51  0.00  0.49 -0.27\n",
    "        # lion  -0.00  0.21 -0.18  0.71 -0.39 -0.52\n",
    "        # love  -0.22  0.42  0.69  0.00  0.41 -0.37\n",
    "        # nyc   -0.75  0.00  0.24 -0.00 -0.52  0.32\n",
    "\n",
    "    vt = pd.DataFrame(vt, index=['d{}'.format(i) for i in range(len(vt))])\n",
    "    if verbosity:\n",
    "        print('VT')\n",
    "        print(vt.round(2))\n",
    "        # VT\n",
    "        #        0     1     2     3     4     5     6     7     8     9     10\n",
    "        # d0  -0.44 -0.44 -0.31 -0.44 -0.44 -0.20 -0.01 -0.01 -0.08 -0.31 -0.01\n",
    "        # d1  -0.09 -0.09  0.19 -0.09 -0.09 -0.09  0.37  0.47  0.56  0.19  0.47\n",
    "        # d2  -0.16 -0.16  0.52 -0.16 -0.16 -0.29 -0.22 -0.32  0.17  0.52 -0.32\n",
    "        # d3   0.00 -0.00  0.00  0.00  0.00  0.00 -0.00  0.71  0.00  0.00 -0.71\n",
    "        # d4  -0.04 -0.04 -0.14 -0.04 -0.04  0.58  0.13 -0.33  0.62 -0.14 -0.33\n",
    "        # d5   0.09  0.09 -0.10  0.09  0.09 -0.51  0.73 -0.27  0.01 -0.10 -0.27\n",
    "        # d6  -0.55  0.24  0.15  0.36 -0.38  0.32  0.32  0.00 -0.32  0.17  0.00\n",
    "        # d7  -0.32  0.46  0.23 -0.64  0.41  0.09  0.09  0.00 -0.09 -0.14  0.00\n",
    "        # d8  -0.52  0.27 -0.24  0.39  0.22 -0.36 -0.36 -0.00  0.36 -0.12  0.00\n",
    "        # d9  -0.14 -0.14 -0.58 -0.14  0.32  0.10  0.10 -0.00 -0.10  0.68 -0.00\n",
    "        # d10 -0.27 -0.63  0.31  0.23  0.55  0.12  0.12 -0.00 -0.12 -0.19 -0.00\n",
    "\n",
    "    # Reconstruct the original term-document matrix.\n",
    "    # The sum of the squares of the error is 0.\n",
    "\n",
    "    return {'u': u, 's': s, 'vt': vt, 'tdm': tdm}\n",
    "\n",
    "\n",
    "def plot_feature_selection(accuracy, title=None):\n",
    "    # accuracy = topic_model['accuracy']\n",
    "    plt.plot(range(len(accuracy)), accuracy)\n",
    "    plt.title(title or 'LSA Model Accuracy')\n",
    "    plt.xlabel('Number of Dimensions Eliminated')\n",
    "    plt.ylabel('Reconstruction Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\"\"\" Some more complicated examples of SVD.\n",
    ">>> import numpy as np\n",
    ">>> u, s, vt = np.linalg.svd(tdm)  # <1>\n",
    "\n",
    ">>> import pandas as pd\n",
    ">>> u = pd.DataFrame(u, index=tdm.index)\n",
    ">>> u.round(2)\n",
    "\n",
    "\n",
    ">>> main('cat dog apple lion NYC love'.lower().split())\n",
    "263it [00:00, 408405.02it/s]\n",
    "             0         1         2         3         4    5    6         7         8         9         10   11\n",
    "nyc    0.674278  0.674278  0.596469  0.674278  0.674278  0.0  0.0  0.000000  0.000000  0.596469  0.000000  0.0\n",
    "lion   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.826567  0.000000  0.000000  0.000000  0.0\n",
    "love   0.000000  0.000000  0.802636  0.000000  0.000000  0.0  0.0  0.000000  0.744190  0.802636  0.000000  0.0\n",
    "dog    0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000  0.826567  0.0\n",
    "apple  0.738477  0.738477  0.000000  0.738477  0.738477  1.0  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
    "cat    0.000000  0.000000  0.000000  0.000000  0.000000  0.0  1.0  0.562839  0.667968  0.000000  0.562839  0.0\n",
    "   nyc lion love dog apple cat                                             text\n",
    "0    1                   1                                NYC is the Big Apple.\n",
    "1    1                   1                       NYC is known as the Big Apple.\n",
    "2    1         1                                                    I love NYC!\n",
    "3    1                   1          I wore a hat to the Big Apple party in NYC.\n",
    "4    1                   1                      Come to NYC. See the Big Apple!\n",
    "5                        1                   Manhattan is called the Big Apple.\n",
    "6                            1          New York is a big city for a small cat.\n",
    "7         1                  1  The lion, a big cat, is the king of the jungle.\n",
    "8              1             1                               I love my pet cat.\n",
    "9    1         1                                    I love New York City (NYC).\n",
    "10                 1         1                          Your dog chased my cat.\n",
    "11                                                     Bright lights, big city?\n",
    "       0   1   2   3   4   5   6   7   8   9   10  11\n",
    "nyc     1   1   1   1   1   0   0   0   0   1   0   0\n",
    "lion    0   0   0   0   0   0   0   1   0   0   0   0\n",
    "love    0   0   1   0   0   0   0   0   1   1   0   0\n",
    "dog     0   0   0   0   0   0   0   0   0   0   1   0\n",
    "apple   1   1   0   1   1   1   0   0   0   0   0   0\n",
    "cat     0   0   0   0   0   0   1   1   1   0   1   0\n",
    "U\n",
    "          0     1     2     3     4     5\n",
    "nyc   -0.75 -0.00  0.24  0.00 -0.52 -0.32\n",
    "lion  -0.00  0.21 -0.18 -0.71 -0.39  0.52\n",
    "love  -0.22  0.42  0.69 -0.00  0.41  0.37\n",
    "dog   -0.00  0.21 -0.18  0.71 -0.39  0.52\n",
    "apple -0.62 -0.21 -0.51 -0.00  0.49  0.27\n",
    "cat   -0.04  0.83 -0.38 -0.00  0.11 -0.38\n",
    "VT\n",
    "       0     1     2     3     4     5     6     7     8     9     10   11\n",
    "d0  -0.44 -0.44 -0.31 -0.44 -0.44 -0.20 -0.01 -0.01 -0.08 -0.31 -0.01  0.0\n",
    "d1  -0.09 -0.09  0.19 -0.09 -0.09 -0.09  0.37  0.47  0.56  0.19  0.47  0.0\n",
    "d2  -0.16 -0.16  0.52 -0.16 -0.16 -0.29 -0.22 -0.32  0.17  0.52 -0.32  0.0\n",
    "d3   0.00  0.00  0.00  0.00  0.00 -0.00 -0.00 -0.71 -0.00  0.00  0.71  0.0\n",
    "d4  -0.04 -0.04 -0.14 -0.04 -0.04  0.58  0.13 -0.33  0.62 -0.14 -0.33  0.0\n",
    "d5  -0.09 -0.09  0.10 -0.09 -0.09  0.51 -0.73  0.27 -0.01  0.10  0.27  0.0\n",
    "d6   0.12 -0.03  0.20 -0.03 -0.51  0.45  0.45  0.00 -0.45  0.25  0.00  0.0\n",
    "d7   0.45 -0.85 -0.04  0.15  0.22  0.02  0.02  0.00 -0.02  0.06  0.00  0.0\n",
    "d8   0.52  0.15 -0.38  0.15 -0.59 -0.24 -0.24  0.00  0.24  0.14  0.00  0.0\n",
    "d9  -0.28 -0.01 -0.62 -0.01  0.22  0.07  0.07  0.00 -0.07  0.69  0.00  0.0\n",
    "d10  0.45  0.15 -0.04 -0.85  0.22  0.02  0.02  0.00 -0.02  0.06  0.00  0.0\n",
    "d11  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  1.0\n",
    "Sigma\n",
    "         d0    d1    d2   d3    d4    d5   d6   d7   d8   d9  d10  d11\n",
    "nyc    3.14  0.00  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "lion   0.00  2.24  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "love   0.00  0.00  1.77  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "dog    0.00  0.00  0.00  1.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "apple  0.00  0.00  0.00  0.0  0.84  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "cat    0.00  0.00  0.00  0.0  0.00  0.52  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "Sigma without zeroing any dim\n",
    "[3.14 2.24 1.77 1.   0.84 0.52]\n",
    "Reconstructed Term-Document Matrix\n",
    "[[ 1.  1.  1.  1.  1.  0. -0. -0. -0.  1. -0.  0.]\n",
    " [-0. -0.  0. -0. -0. -0.  0.  1.  0.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.  0. -0. -0.  1.  1. -0.  0.]\n",
    " [-0. -0.  0. -0. -0. -0.  0.  0.  0.  0.  1.  0.]\n",
    " [ 1.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.]\n",
    " [-0. -0.  0. -0. -0. -0.  1.  1.  1.  0.  1.  0.]]\n",
    "Error without reducing dimensions\n",
    "1.5846963069762592e-15\n",
    "Sigma after zeroing out dim 5\n",
    "[3.14 2.24 1.77 1.   0.84 0.  ]\n",
    "Error after zeroing out dim 5\n",
    "0.0614920369471735\n",
    "Sigma after zeroing out dim 4\n",
    "[3.14 2.24 1.77 1.   0.   0.  ]\n",
    "Error after zeroing out dim 4\n",
    "0.11683863316404541\n",
    "Sigma after zeroing out dim 3\n",
    "[3.14 2.24 1.77 0.   0.   0.  ]\n",
    "Error after zeroing out dim 3\n",
    "0.1659522675004209\n",
    "Sigma after zeroing out dim 2\n",
    "[3.14 2.24 0.   0.   0.   0.  ]\n",
    "Error after zeroing out dim 2\n",
    "0.2667342349285279\n",
    "Sigma after zeroing out dim 1\n",
    "[3.14 0.   0.   0.   0.   0.  ]\n",
    "Error after zeroing out dim 1\n",
    "0.3749554593913143\n",
    "[1.         0.93850796 0.88316137 0.83404773 0.73326577 0.62504454]\n",
    "             0         1         2         3         4    5    6         7         8         9         10   11\n",
    "nyc    0.674278  0.674278  0.596469  0.674278  0.674278  0.0  0.0  0.000000  0.000000  0.596469  0.000000  0.0\n",
    "lion   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.826567  0.000000  0.000000  0.000000  0.0\n",
    "love   0.000000  0.000000  0.802636  0.000000  0.000000  0.0  0.0  0.000000  0.744190  0.802636  0.000000  0.0\n",
    "dog    0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000  0.826567  0.0\n",
    "apple  0.738477  0.738477  0.000000  0.738477  0.738477  1.0  0.0  0.000000  0.000000  0.000000  0.000000  0.0\n",
    "cat    0.000000  0.000000  0.000000  0.000000  0.000000  0.0  1.0  0.562839  0.667968  0.000000  0.562839  0.0\n",
    "U\n",
    "          0     1     2     3     4     5\n",
    "nyc   -0.66  0.07 -0.27 -0.00  0.51 -0.47\n",
    "lion  -0.00  0.18  0.21  0.71  0.48  0.44\n",
    "love  -0.21  0.54 -0.64  0.00 -0.30  0.41\n",
    "dog   -0.00  0.18  0.21 -0.71  0.48  0.44\n",
    "apple -0.72 -0.26  0.40 -0.00 -0.38  0.33\n",
    "cat   -0.04  0.76  0.52 -0.00 -0.19 -0.35\n",
    "VT\n",
    "       0     1     2     3     4     5     6     7     8     9     10   11\n",
    "d0  -0.44 -0.44 -0.25 -0.44 -0.44 -0.32 -0.02 -0.01 -0.08 -0.25 -0.01  0.0\n",
    "d1  -0.09 -0.09  0.29 -0.09 -0.09 -0.16  0.46  0.35  0.56  0.29  0.35  0.0\n",
    "d2   0.08  0.08 -0.50  0.08  0.08  0.30  0.38  0.34 -0.09 -0.50  0.34  0.0\n",
    "d3  -0.00 -0.00  0.00 -0.00 -0.00  0.00 -0.00  0.71  0.00  0.00 -0.71  0.0\n",
    "d4   0.09  0.09  0.10  0.09  0.09 -0.54 -0.27  0.41 -0.49  0.10  0.41  0.0\n",
    "d5  -0.13 -0.13  0.08 -0.13 -0.13  0.58 -0.62  0.30  0.13  0.08  0.30  0.0\n",
    "d6   0.22  0.01  0.20  0.01 -0.64  0.30  0.33  0.00 -0.49  0.26  0.00  0.0\n",
    "d7   0.46 -0.85 -0.03  0.15  0.21  0.02  0.02  0.00 -0.03  0.06  0.00  0.0\n",
    "d8   0.49  0.17 -0.42  0.17 -0.49 -0.24 -0.27  0.00  0.40  0.05  0.00  0.0\n",
    "d9  -0.24 -0.01 -0.61 -0.01  0.17  0.07  0.07  0.00 -0.11  0.72  0.00  0.0\n",
    "d10  0.46  0.15 -0.03 -0.85  0.21  0.02  0.02  0.00 -0.03  0.06  0.00  0.0\n",
    "d11  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  1.0\n",
    "Sigma\n",
    "         d0    d1    d2    d3    d4    d5   d6   d7   d8   d9  d10  d11\n",
    "nyc    2.24  0.00  0.00  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "lion   0.00  1.63  0.00  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "love   0.00  0.00  1.36  0.00  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "dog    0.00  0.00  0.00  0.83  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "apple  0.00  0.00  0.00  0.00  0.71  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "cat    0.00  0.00  0.00  0.00  0.00  0.56  0.0  0.0  0.0  0.0  0.0  0.0\n",
    "Sigma without zeroing any dim\n",
    "[2.24 1.63 1.36 0.83 0.71 0.56]\n",
    "Reconstructed Term-Document Matrix\n",
    "[[ 0.67  0.67  0.6   0.67  0.67  0.   -0.    0.    0.    0.6   0.    0.  ]\n",
    " [ 0.    0.   -0.    0.    0.    0.    0.    0.83  0.   -0.   -0.    0.  ]\n",
    " [ 0.    0.    0.8   0.    0.   -0.    0.    0.    0.74  0.8   0.    0.  ]\n",
    " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.83  0.  ]\n",
    " [ 0.74  0.74  0.    0.74  0.74  1.   -0.   -0.    0.   -0.   -0.    0.  ]\n",
    " [-0.   -0.    0.   -0.   -0.    0.    1.    0.56  0.67  0.    0.56  0.  ]]\n",
    "Error without reducing dimensions\n",
    "2.4118514282911836e-16\n",
    "Sigma after zeroing out dim 5\n",
    "[2.24 1.63 1.36 0.83 0.71 0.  ]\n",
    "Error after zeroing out dim 5\n",
    "0.06622832691855576\n",
    "Sigma after zeroing out dim 4\n",
    "[2.24 1.63 1.36 0.83 0.   0.  ]\n",
    "Error after zeroing out dim 4\n",
    "0.10640521215974559\n",
    "Sigma after zeroing out dim 3\n",
    "[2.24 1.63 1.36 0.   0.   0.  ]\n",
    "Error after zeroing out dim 3\n",
    "0.14426065117971226\n",
    "Sigma after zeroing out dim 2\n",
    "[2.24 1.63 0.   0.   0.   0.  ]\n",
    "Error after zeroing out dim 2\n",
    "0.21538404738257216\n",
    "Sigma after zeroing out dim 1\n",
    "[2.24 0.   0.   0.   0.   0.  ]\n",
    "Error after zeroing out dim 1\n",
    "0.28856858954287745\n",
    "[1.         0.93377167 0.89359479 0.85573935 0.78461595 0.71143141]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def lsa_models(vocabulary='cat dog apple lion NYC love'.lower().split(), docs=11, verbosity=0):\n",
    "    # vocabulary = 'cat dog apple lion NYC love big small bright'.lower().split()\n",
    "    if isinstance(docs, int):\n",
    "        docs = raw_datas[:docs]\n",
    "    tdm, tfidfdm, tfidfer = docs_to_tdm(docs=docs, vocabulary=vocabulary)\n",
    "    lsa_bow_model = lsa(tdm)  # (tdm - tdm.mean(axis=1)) # SVD fails to converge if you center, like PCA does\n",
    "    lsa_bow_model['vocabulary'] = tdm.index.values\n",
    "    lsa_bow_model['docs'] = docs\n",
    "    err = accuracy_study(verbosity=verbosity, **lsa_bow_model)\n",
    "    lsa_bow_model['err'] = err\n",
    "    lsa_bow_model['accuracy'] = list(1. - np.array(err))\n",
    "    \n",
    "    lsa_tfidf_model = lsa(tdm=tfidfdm)\n",
    "    lsa_bow_model['vocabulary'] = tfidfdm.index.values\n",
    "    lsa_tfidf_model['docs'] = docs\n",
    "    err = accuracy_study(verbosity=verbosity, **lsa_tfidf_model)\n",
    "    lsa_tfidf_model['err'] = err\n",
    "    lsa_tfidf_model['accuracy'] = list(1. - np.array(err))\n",
    "\n",
    "    return lsa_bow_model, lsa_tfidf_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    numdocs = 11\n",
    "    with open('cats_and_dogs_sorted.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        raw_datas =[]\n",
    "        for line in f:\n",
    "            raw_datas.append(line.replace(\"\\n\", \"\"))\n",
    "    docs = raw_datas[:numdocs]\n",
    "    vocabulary = sys.argv[1:] or 'cat dog apple lion NYC love'.lower().split()\n",
    "    lsa_bow_model, lsa_tfidf_model = lsa_models(vocabulary=vocabulary, docs=docs)\n",
    "    tdm = lsa_bow_model['tdm']\n",
    "    tfidfdm = lsa_tfidf_model['tdm']\n",
    "    print(prettify_tdm(tdm=tdm, docs=docs, vocabulary=vocabulary))\n",
    "    acc = plot_feature_selection(accuracy=lsa_bow_model['accuracy'])\n",
    "    print(\"BOW accuracy after multiplying Truncated SVD back together\")\n",
    "    print(acc)\n",
    "    acc = plot_feature_selection(accuracy=lsa_tfidf_model['accuracy'], title='TF-IDF LSA Model Accuracy')\n",
    "    print(\"TF-IDF accuracy after multiplying Truncated SVD back together\")\n",
    "    print(acc)\n",
    "    plt.legend(['BOW Reconstruction Accuracy', 'TF-IDF Reconstruction Accuracy'])\n",
    "    yn = input('Would you like to see the accuracy comparison plot? [y]')\n",
    "    if not yn or yn.lower().startswith('y'):\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>apple</th>\n",
       "      <th>lion</th>\n",
       "      <th>nyc</th>\n",
       "      <th>love</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NYC is the Big Apple.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NYC is known as the Big Apple.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I love NYC!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>I wore a hat to the Big Apple party in NYC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Come to NYC. See the Big Apple!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Manhattan is called the Big Apple.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>New York is a big city for a small cat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The lion, a big cat, is the king of the jungle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>I love my pet cat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I love New York City (NYC).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Your dog chased my cat.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat dog apple lion nyc love                                             text\n",
       "0              1        1                                 NYC is the Big Apple.\n",
       "1              1        1                        NYC is known as the Big Apple.\n",
       "2                       1    1                                      I love NYC!\n",
       "3              1        1           I wore a hat to the Big Apple party in NYC.\n",
       "4              1        1                       Come to NYC. See the Big Apple!\n",
       "5              1                             Manhattan is called the Big Apple.\n",
       "6    1                                  New York is a big city for a small cat.\n",
       "7    1              1           The lion, a big cat, is the king of the jungle.\n",
       "8    1                       1                               I love my pet cat.\n",
       "9                       1    1                      I love New York City (NYC).\n",
       "10   1   1                                              Your dog chased my cat."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_svd, tfidf_svd = lsa_models()\n",
    "prettify_tdm(**bow_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1   2   3   4   5   6   7   8   9   10\n",
       "cat     0   0   0   0   0   0   1   1   1   0   1\n",
       "dog     0   0   0   0   0   0   0   0   0   0   1\n",
       "apple   1   1   0   1   1   1   0   0   0   0   0\n",
       "lion    0   0   0   0   0   0   0   1   0   0   0\n",
       "nyc     1   1   1   1   1   0   0   0   0   1   0\n",
       "love    0   0   1   0   0   0   0   0   1   1   0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = bow_svd['tdm']\n",
    "tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyc</th>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3     4     5\n",
       "cat   -0.04  0.83 -0.38 -0.00  0.11 -0.38\n",
       "dog   -0.00  0.21 -0.18 -0.71 -0.39  0.52\n",
       "apple -0.62 -0.21 -0.51  0.00  0.49  0.27\n",
       "lion  -0.00  0.21 -0.18  0.71 -0.39  0.52\n",
       "nyc   -0.75  0.00  0.24 -0.00 -0.52 -0.32\n",
       "love  -0.22  0.42  0.69  0.00  0.41  0.37"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "U, s, Vt = np.linalg.svd(tdm)\n",
    "import pandas as pd\n",
    "pd.DataFrame(U, index=tdm.index).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.1, 2.2, 1.8, 1. , 0.8, 0.5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    10\n",
       "0  3.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  2.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2  0.0  0.0  1.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  0.8  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.zeros((len(U), len(Vt)))\n",
    "pd.np.fill_diagonal(S, s)\n",
    "pd.DataFrame(S).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     10\n",
       "0  -0.44 -0.44 -0.31 -0.44 -0.44 -0.20 -0.01 -0.01 -0.08 -0.31 -0.01\n",
       "1  -0.09 -0.09  0.19 -0.09 -0.09 -0.09  0.37  0.47  0.56  0.19  0.47\n",
       "2  -0.16 -0.16  0.52 -0.16 -0.16 -0.29 -0.22 -0.32  0.17  0.52 -0.32\n",
       "3   0.00 -0.00 -0.00  0.00  0.00  0.00 -0.00  0.71 -0.00 -0.00 -0.71\n",
       "4  -0.04 -0.04 -0.14 -0.04 -0.04  0.58  0.13 -0.33  0.62 -0.14 -0.33\n",
       "5  -0.09 -0.09  0.10 -0.09 -0.09  0.51 -0.73  0.27 -0.01  0.10  0.27\n",
       "6  -0.57  0.21  0.11  0.33 -0.31  0.34  0.34  0.00 -0.34  0.23  0.00\n",
       "7  -0.32  0.47  0.25 -0.63  0.41  0.07  0.07  0.00 -0.07 -0.18  0.00\n",
       "8  -0.50  0.29 -0.20  0.41  0.16 -0.37 -0.37 -0.00  0.37 -0.17  0.00\n",
       "9  -0.15 -0.15 -0.59 -0.15  0.42  0.04  0.04 -0.00 -0.04  0.63 -0.00\n",
       "10 -0.26 -0.62  0.33  0.24  0.54  0.09  0.09  0.00 -0.09 -0.23 -0.00"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Vt).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06, 0.12, 0.17, 0.28, 0.39, 0.55])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = []\n",
    "for numdim in range(len(s), 0, -1):\n",
    "    S[numdim - 1, numdim - 1] = 0\n",
    "    reconstructed_tdm = U.dot(S).dot(Vt)\n",
    "    err.append(np.sqrt(((\\\n",
    "    reconstructed_tdm - tdm).values.flatten() ** 2).sum()/ np.product(tdm.shape)))\n",
    "np.array(err).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Anaconda\\\\python\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\pointcloud.csv.gz',), **{'nrows': None, 'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 6)\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from nlpia.data.loaders import get_data\n",
    "df = get_data('pointcloud').sample(1000)\n",
    "pca = PCA(n_components=2)\n",
    "df2d = pd.DataFrame(pca.fit_transform(df), columns=list('xy'))\n",
    "df2d.plot(kind='scatter', x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Anaconda\\\\python\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'nrows': None, 'low_memory': False})`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       spam                                                    text\n",
       "sms0      0  Go until jurong point, crazy.. Available only in bu...\n",
       "sms1      0                           Ok lar... Joking wif u oni...\n",
       "sms2!     1  Free entry in 2 a wkly comp to win FA Cup final tkt...\n",
       "sms3      0       U dun say so early hor... U c already then say...\n",
       "sms4      0  Nah I don't think he goes to usf, he lives around h...\n",
       "sms5!     1  FreeMsg Hey there darling it's been 3 week's now an..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nlpia.data.loaders import get_data\n",
    "pd.options.display.width = 120\n",
    "sms = get_data('sms-spam')\n",
    "index = ['sms{}{}'.format(i, '!'*j)\n",
    "         for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms.index = index\n",
    "sms.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9232"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4837, 9232)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_docs = pd.DataFrame(tfidf_docs)\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean()\n",
    "tfidf_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.spam.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>...</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.201</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  ...  topic13  topic14  topic15\n",
       "sms0    0.201   0.003   0.037  ...   -0.032   -0.004    0.041\n",
       "sms1    0.404  -0.094  -0.077  ...   -0.028    0.046   -0.040\n",
       "sms2!  -0.030  -0.048   0.090  ...   -0.010   -0.039    0.062\n",
       "sms3    0.329  -0.033  -0.035  ...   -0.054    0.014   -0.074\n",
       "sms4    0.002   0.031   0.038  ...    0.030   -0.092   -0.025\n",
       "sms5!  -0.016   0.059   0.014  ...    0.083    0.012    0.001\n",
       "\n",
       "[6 rows x 16 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=16)\n",
    "pca = pca.fit(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca.n_components)]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns,index=index)\n",
    "pca_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go': 3807,\n",
       " 'until': 8487,\n",
       " 'jurong': 4675,\n",
       " 'point': 6296,\n",
       " ',': 13,\n",
       " 'crazy': 2549,\n",
       " '..': 21,\n",
       " 'available': 1531,\n",
       " 'only': 5910,\n",
       " 'in': 4396,\n",
       " 'bugis': 1973,\n",
       " 'n': 5594,\n",
       " 'great': 3894,\n",
       " 'world': 8977,\n",
       " 'la': 4811,\n",
       " 'e': 3056,\n",
       " 'buffet': 1971,\n",
       " '...': 25,\n",
       " 'cine': 2277,\n",
       " 'there': 8071,\n",
       " 'got': 3855,\n",
       " 'amore': 1296,\n",
       " 'wat': 8736,\n",
       " 'ok': 5874,\n",
       " 'lar': 4848,\n",
       " 'joking': 4642,\n",
       " 'wif': 8875,\n",
       " 'u': 8395,\n",
       " 'oni': 5906,\n",
       " 'free': 3604,\n",
       " 'entry': 3195,\n",
       " '2': 471,\n",
       " 'a': 1054,\n",
       " 'wkly': 8933,\n",
       " 'comp': 2386,\n",
       " 'to': 8192,\n",
       " 'win': 8890,\n",
       " 'fa': 3328,\n",
       " 'cup': 2608,\n",
       " 'final': 3450,\n",
       " 'tkts': 8180,\n",
       " '21st': 497,\n",
       " 'may': 5272,\n",
       " '2005': 487,\n",
       " '.': 15,\n",
       " 'text': 8020,\n",
       " '87121': 948,\n",
       " 'receive': 6688,\n",
       " 'question': 6574,\n",
       " '(': 9,\n",
       " 'std': 7651,\n",
       " 'txt': 8379,\n",
       " 'rate': 6628,\n",
       " ')': 10,\n",
       " 't': 7889,\n",
       " '&': 7,\n",
       " \"c's\": 2020,\n",
       " 'apply': 1383,\n",
       " '08452810075': 115,\n",
       " 'over': 6003,\n",
       " '18': 438,\n",
       " \"'\": 8,\n",
       " 's': 6959,\n",
       " 'dun': 3041,\n",
       " 'say': 7034,\n",
       " 'so': 7438,\n",
       " 'early': 3069,\n",
       " 'hor': 4207,\n",
       " 'c': 2019,\n",
       " 'already': 1268,\n",
       " 'then': 8065,\n",
       " 'nah': 5606,\n",
       " 'i': 4311,\n",
       " \"don't\": 2948,\n",
       " 'think': 8092,\n",
       " 'he': 4048,\n",
       " 'goes': 3819,\n",
       " 'usf': 8537,\n",
       " 'lives': 5004,\n",
       " 'around': 1435,\n",
       " 'here': 4104,\n",
       " 'though': 8111,\n",
       " 'freemsg': 3613,\n",
       " 'hey': 4116,\n",
       " 'darling': 2666,\n",
       " \"it's\": 4535,\n",
       " 'been': 1693,\n",
       " '3': 591,\n",
       " \"week's\": 8788,\n",
       " 'now': 5784,\n",
       " 'and': 1310,\n",
       " 'no': 5732,\n",
       " 'word': 8967,\n",
       " 'back': 1584,\n",
       " '!': 0,\n",
       " \"i'd\": 4312,\n",
       " 'like': 4954,\n",
       " 'some': 7454,\n",
       " 'fun': 3677,\n",
       " 'you': 9158,\n",
       " 'up': 8489,\n",
       " 'for': 3552,\n",
       " 'it': 4533,\n",
       " 'still': 7674,\n",
       " '?': 1037,\n",
       " 'tb': 7955,\n",
       " 'xxx': 9097,\n",
       " 'chgs': 2230,\n",
       " 'send': 7127,\n",
       " '£': 9216,\n",
       " '1.50': 344,\n",
       " 'rcv': 6641,\n",
       " 'even': 3240,\n",
       " 'my': 5584,\n",
       " 'brother': 1942,\n",
       " 'is': 4519,\n",
       " 'not': 5769,\n",
       " 'speak': 7529,\n",
       " 'with': 8918,\n",
       " 'me': 5281,\n",
       " 'they': 8083,\n",
       " 'treat': 8312,\n",
       " 'aids': 1214,\n",
       " 'patent': 6106,\n",
       " 'as': 1452,\n",
       " 'per': 6148,\n",
       " 'your': 9171,\n",
       " 'request': 6796,\n",
       " 'melle': 5315,\n",
       " 'oru': 5968,\n",
       " 'minnaminunginte': 5386,\n",
       " 'nurungu': 5807,\n",
       " 'vettam': 8599,\n",
       " 'has': 4022,\n",
       " 'set': 7154,\n",
       " 'callertune': 2047,\n",
       " 'all': 1253,\n",
       " 'callers': 2046,\n",
       " 'press': 6418,\n",
       " '*': 11,\n",
       " '9': 982,\n",
       " 'copy': 2489,\n",
       " 'friends': 3634,\n",
       " 'winner': 8900,\n",
       " 'valued': 8569,\n",
       " 'network': 5678,\n",
       " 'customer': 2620,\n",
       " 'have': 4036,\n",
       " 'selected': 7113,\n",
       " 'receivea': 6689,\n",
       " '900': 986,\n",
       " 'prize': 6450,\n",
       " 'reward': 6851,\n",
       " 'claim': 2283,\n",
       " 'call': 2038,\n",
       " '09061701461': 263,\n",
       " 'code': 2344,\n",
       " 'kl341': 4771,\n",
       " 'valid': 8565,\n",
       " '12': 384,\n",
       " 'hours': 4226,\n",
       " 'had': 3965,\n",
       " 'mobile': 5441,\n",
       " '11': 371,\n",
       " 'months': 5484,\n",
       " 'or': 5946,\n",
       " 'more': 5489,\n",
       " 'r': 6590,\n",
       " 'entitled': 3192,\n",
       " 'update': 8495,\n",
       " 'the': 8052,\n",
       " 'latest': 4862,\n",
       " 'colour': 2364,\n",
       " 'mobiles': 5442,\n",
       " 'camera': 2058,\n",
       " 'co': 2333,\n",
       " 'on': 5897,\n",
       " '08002986030': 99,\n",
       " \"i'm\": 4314,\n",
       " 'gonna': 3834,\n",
       " 'be': 1669,\n",
       " 'home': 4176,\n",
       " 'soon': 7483,\n",
       " 'want': 8715,\n",
       " 'talk': 7921,\n",
       " 'about': 1076,\n",
       " 'this': 8100,\n",
       " 'stuff': 7741,\n",
       " 'anymore': 1350,\n",
       " 'tonight': 8235,\n",
       " 'k': 4683,\n",
       " \"i've\": 4316,\n",
       " 'cried': 2566,\n",
       " 'enough': 3182,\n",
       " 'today': 8199,\n",
       " 'six': 7341,\n",
       " 'chances': 2172,\n",
       " 'cash': 2116,\n",
       " 'from': 3652,\n",
       " '100': 354,\n",
       " '20,000': 482,\n",
       " 'pounds': 6357,\n",
       " '>': 1035,\n",
       " 'csh': 2584,\n",
       " '87575': 952,\n",
       " 'cost': 2501,\n",
       " '150p': 415,\n",
       " '/': 27,\n",
       " 'day': 2683,\n",
       " '6days': 827,\n",
       " '16': 431,\n",
       " '+': 12,\n",
       " 'tsandcs': 8344,\n",
       " 'reply': 6788,\n",
       " 'hl': 4148,\n",
       " '4': 659,\n",
       " 'info': 4433,\n",
       " 'urgent': 8513,\n",
       " 'won': 8950,\n",
       " '1': 337,\n",
       " 'week': 8787,\n",
       " 'membership': 5321,\n",
       " 'our': 5980,\n",
       " '100,000': 355,\n",
       " 'jackpot': 4564,\n",
       " ':': 1006,\n",
       " '81010': 900,\n",
       " 'www.dbuk.net': 9039,\n",
       " 'lccltd': 4880,\n",
       " 'pobox': 6286,\n",
       " '4403ldnw1a7rw18': 696,\n",
       " 'searching': 7081,\n",
       " 'right': 6863,\n",
       " 'words': 8968,\n",
       " 'thank': 8037,\n",
       " 'breather': 1912,\n",
       " 'promise': 6487,\n",
       " 'wont': 8958,\n",
       " 'take': 7913,\n",
       " 'help': 4089,\n",
       " 'granted': 3883,\n",
       " 'will': 8887,\n",
       " 'fulfil': 3673,\n",
       " 'wonderful': 8955,\n",
       " 'blessing': 1802,\n",
       " 'at': 1488,\n",
       " 'times': 8158,\n",
       " 'date': 2675,\n",
       " 'sunday': 7805,\n",
       " 'xxxmobilemovieclub': 9098,\n",
       " 'use': 8531,\n",
       " 'credit': 2556,\n",
       " 'click': 2306,\n",
       " 'wap': 8719,\n",
       " 'link': 4977,\n",
       " 'next': 5696,\n",
       " 'message': 5340,\n",
       " 'http://wap': 4259,\n",
       " 'xxxmobilemovieclub.com': 9099,\n",
       " '=': 1031,\n",
       " 'qjkgighjjgcbl': 6566,\n",
       " 'oh': 5869,\n",
       " 'watching': 8743,\n",
       " ':)': 1008,\n",
       " 'eh': 3116,\n",
       " 'remember': 6755,\n",
       " 'how': 4233,\n",
       " 'spell': 7545,\n",
       " 'his': 4139,\n",
       " 'name': 5612,\n",
       " 'yes': 9137,\n",
       " 'did': 2823,\n",
       " 'v': 8553,\n",
       " 'naughty': 5635,\n",
       " 'make': 5193,\n",
       " 'wet': 8828,\n",
       " 'fine': 3458,\n",
       " 'if': 4350,\n",
       " 'that': 8045,\n",
       " '\\x92': 9211,\n",
       " 'way': 8753,\n",
       " 'feel': 3400,\n",
       " 'its': 4546,\n",
       " 'gota': 3856,\n",
       " 'b': 1560,\n",
       " 'england': 3173,\n",
       " 'macedonia': 5156,\n",
       " '-': 14,\n",
       " 'dont': 2952,\n",
       " 'miss': 5402,\n",
       " 'goals': 3812,\n",
       " 'team': 7968,\n",
       " 'news': 5692,\n",
       " 'ur': 8510,\n",
       " 'national': 5629,\n",
       " '87077': 947,\n",
       " 'eg': 3109,\n",
       " 'try': 8340,\n",
       " 'wales': 8695,\n",
       " 'scotland': 7060,\n",
       " '4txt': 737,\n",
       " 'ú1': 9220,\n",
       " '20': 481,\n",
       " 'poboxox': 6287,\n",
       " '36504w45wq': 629,\n",
       " 'seriously': 7147,\n",
       " '‘': 9225,\n",
       " 'm': 5139,\n",
       " 'going': 3823,\n",
       " 'ha': 3961,\n",
       " 'ü': 9221,\n",
       " 'pay': 6119,\n",
       " 'first': 3476,\n",
       " 'when': 8840,\n",
       " 'da': 2639,\n",
       " 'stock': 7678,\n",
       " 'comin': 2376,\n",
       " 'aft': 1182,\n",
       " 'finish': 3462,\n",
       " 'lunch': 5121,\n",
       " 'str': 7702,\n",
       " 'down': 2974,\n",
       " 'lor': 5058,\n",
       " 'ard': 1410,\n",
       " 'smth': 7422,\n",
       " 'ffffffffff': 3420,\n",
       " 'alright': 1269,\n",
       " 'can': 2062,\n",
       " 'meet': 5303,\n",
       " 'sooner': 7485,\n",
       " 'just': 4677,\n",
       " 'forced': 3554,\n",
       " 'myself': 5591,\n",
       " 'eat': 3081,\n",
       " 'slice': 7372,\n",
       " 'really': 6670,\n",
       " 'hungry': 4287,\n",
       " 'tho': 8107,\n",
       " 'sucks': 7778,\n",
       " 'mark': 5230,\n",
       " 'getting': 3767,\n",
       " 'worried': 8981,\n",
       " 'knows': 4782,\n",
       " 'sick': 7289,\n",
       " 'turn': 8362,\n",
       " 'pizza': 6238,\n",
       " 'lol': 5035,\n",
       " 'always': 1279,\n",
       " 'convincing': 2476,\n",
       " 'catch': 2128,\n",
       " 'bus': 1993,\n",
       " 'are': 1411,\n",
       " 'frying': 3660,\n",
       " 'an': 1305,\n",
       " 'egg': 3111,\n",
       " 'tea': 7962,\n",
       " 'eating': 3084,\n",
       " \"mom's\": 5463,\n",
       " 'left': 4901,\n",
       " 'dinner': 2858,\n",
       " 'do': 2909,\n",
       " 'love': 5081,\n",
       " \"we're\": 8760,\n",
       " 'packing': 6032,\n",
       " 'car': 2085,\n",
       " \"i'll\": 4313,\n",
       " 'let': 4923,\n",
       " 'know': 4779,\n",
       " \"there's\": 8074,\n",
       " 'room': 6906,\n",
       " 'ahhh': 1209,\n",
       " 'work': 8970,\n",
       " 'vaguely': 8560,\n",
       " 'what': 8832,\n",
       " 'does': 2923,\n",
       " 'wait': 8689,\n",
       " \"that's\": 8048,\n",
       " 'clear': 2300,\n",
       " 'were': 8817,\n",
       " 'sure': 7832,\n",
       " 'being': 1713,\n",
       " 'sarcastic': 7010,\n",
       " 'why': 8868,\n",
       " 'x': 9076,\n",
       " \"doesn't\": 2926,\n",
       " 'live': 5000,\n",
       " 'us': 8525,\n",
       " 'yeah': 9125,\n",
       " 'was': 8728,\n",
       " 'apologetic': 1371,\n",
       " 'fallen': 3352,\n",
       " 'out': 5983,\n",
       " 'she': 7199,\n",
       " 'actin': 1123,\n",
       " 'spoilt': 7572,\n",
       " 'child': 2238,\n",
       " 'caught': 2132,\n",
       " 'till': 8152,\n",
       " 'but': 1999,\n",
       " 'we': 8757,\n",
       " \"won't\": 8951,\n",
       " 'doing': 2938,\n",
       " 'too': 8242,\n",
       " 'badly': 1589,\n",
       " 'cheers': 2214,\n",
       " 'tell': 7985,\n",
       " 'anything': 1356,\n",
       " 'fear': 3391,\n",
       " 'of': 5847,\n",
       " 'fainting': 3344,\n",
       " 'housework': 4231,\n",
       " 'quick': 6577,\n",
       " 'cuppa': 2610,\n",
       " 'thanks': 8038,\n",
       " 'subscription': 7766,\n",
       " 'ringtone': 6872,\n",
       " 'uk': 8415,\n",
       " 'charged': 2184,\n",
       " '5': 745,\n",
       " 'month': 5479,\n",
       " 'please': 6265,\n",
       " 'confirm': 2431,\n",
       " 'by': 2016,\n",
       " 'replying': 6790,\n",
       " 'yup': 9192,\n",
       " 'look': 5046,\n",
       " 'timings': 8162,\n",
       " 'msg': 5528,\n",
       " 'again': 1190,\n",
       " 'xuhui': 9093,\n",
       " 'learn': 4892,\n",
       " '2nd': 567,\n",
       " 'her': 4099,\n",
       " 'lesson': 4921,\n",
       " '8am': 974,\n",
       " 'oops': 5921,\n",
       " \"roommate's\": 6909,\n",
       " 'done': 2950,\n",
       " 'see': 7098,\n",
       " 'letter': 4926,\n",
       " 'decide': 2711,\n",
       " 'hello': 4084,\n",
       " \"how's\": 4235,\n",
       " 'saturday': 7024,\n",
       " 'texting': 8027,\n",
       " \"you'd\": 9159,\n",
       " 'decided': 2712,\n",
       " 'tomo': 8224,\n",
       " 'trying': 8342,\n",
       " 'invite': 4493,\n",
       " 'pls': 6273,\n",
       " 'ahead': 1208,\n",
       " 'watts': 8751,\n",
       " 'wanted': 8716,\n",
       " 'weekend': 8791,\n",
       " 'abiola': 1072,\n",
       " 'forget': 3560,\n",
       " 'need': 5654,\n",
       " 'crave': 2546,\n",
       " 'most': 5499,\n",
       " 'sweet': 7863,\n",
       " 'arabian': 1407,\n",
       " 'steed': 7658,\n",
       " 'mmmmmm': 5431,\n",
       " 'yummy': 9187,\n",
       " '07732584351': 62,\n",
       " 'rodger': 6895,\n",
       " 'burns': 1990,\n",
       " 'tried': 8321,\n",
       " 're': 6645,\n",
       " 'sms': 7416,\n",
       " 'nokia': 5744,\n",
       " 'camcorder': 2056,\n",
       " '08000930705': 95,\n",
       " 'delivery': 2750,\n",
       " 'tomorrow': 8226,\n",
       " 'who': 8859,\n",
       " 'seeing': 7101,\n",
       " 'hope': 4198,\n",
       " 'man': 5203,\n",
       " 'well': 8807,\n",
       " 'endowed': 3163,\n",
       " 'am': 1281,\n",
       " '<#>': 1024,\n",
       " 'inches': 4401,\n",
       " 'calls': 2053,\n",
       " 'messages': 5344,\n",
       " 'missed': 5405,\n",
       " \"didn't\": 2828,\n",
       " 'get': 3760,\n",
       " 'hep': 4098,\n",
       " 'immunisation': 4379,\n",
       " 'nigeria': 5708,\n",
       " 'fair': 3345,\n",
       " 'hopefully': 4201,\n",
       " 'tyler': 8389,\n",
       " \"can't\": 2063,\n",
       " 'could': 2511,\n",
       " 'maybe': 5274,\n",
       " 'ask': 1463,\n",
       " 'bit': 1779,\n",
       " 'stubborn': 7730,\n",
       " 'hospital': 4214,\n",
       " 'kept': 4730,\n",
       " 'telling': 7986,\n",
       " 'weak': 8762,\n",
       " 'sucker': 7776,\n",
       " 'hospitals': 4215,\n",
       " 'suckers': 7777,\n",
       " 'thinked': 8093,\n",
       " 'time': 8154,\n",
       " 'saw': 7033,\n",
       " 'class': 2292,\n",
       " 'gram': 3875,\n",
       " 'usually': 8543,\n",
       " 'runs': 6949,\n",
       " 'half': 3977,\n",
       " 'eighth': 3119,\n",
       " 'smarter': 7395,\n",
       " 'gets': 3763,\n",
       " 'almost': 1264,\n",
       " 'whole': 8862,\n",
       " 'second': 7085,\n",
       " 'fyi': 3693,\n",
       " 'ride': 6862,\n",
       " 'morning': 5493,\n",
       " \"he's\": 4050,\n",
       " 'crashing': 2545,\n",
       " 'place': 6240,\n",
       " 'wow': 8997,\n",
       " 'never': 5683,\n",
       " 'realized': 6668,\n",
       " 'embarassed': 3144,\n",
       " 'accomodations': 1103,\n",
       " 'thought': 8112,\n",
       " 'liked': 4955,\n",
       " 'since': 7314,\n",
       " 'best': 1733,\n",
       " 'seemed': 7105,\n",
       " 'happy': 4011,\n",
       " '\"': 1,\n",
       " 'cave': 2136,\n",
       " 'sorry': 7494,\n",
       " 'give': 3788,\n",
       " 'offered': 5855,\n",
       " 'embarassing': 3145,\n",
       " 'ac': 1089,\n",
       " 'sptv': 7594,\n",
       " 'new': 5687,\n",
       " 'jersey': 4608,\n",
       " 'devils': 2803,\n",
       " 'detroit': 2797,\n",
       " 'red': 6711,\n",
       " 'wings': 8898,\n",
       " 'play': 6255,\n",
       " 'ice': 4330,\n",
       " 'hockey': 4161,\n",
       " 'correct': 2494,\n",
       " 'incorrect': 4412,\n",
       " 'end': 3158,\n",
       " 'mallika': 5202,\n",
       " 'sherawat': 7208,\n",
       " 'yesterday': 9141,\n",
       " 'find': 3455,\n",
       " '@': 1038,\n",
       " '<url>': 1030,\n",
       " 'congrats': 2437,\n",
       " 'year': 9126,\n",
       " 'special': 7531,\n",
       " 'cinema': 2278,\n",
       " 'pass': 6094,\n",
       " 'yours': 9176,\n",
       " '09061209465': 258,\n",
       " 'suprman': 7830,\n",
       " 'matrix': 5263,\n",
       " 'starwars': 7638,\n",
       " 'etc': 3230,\n",
       " 'bx420': 2014,\n",
       " 'ip4': 4502,\n",
       " '5we': 781,\n",
       " '150pm': 417,\n",
       " 'later': 4861,\n",
       " 'meeting': 5305,\n",
       " 'where': 8846,\n",
       " 'reached': 6652,\n",
       " 'gauti': 3728,\n",
       " 'sehwag': 7110,\n",
       " 'odi': 5846,\n",
       " 'series': 7145,\n",
       " 'pick': 6211,\n",
       " '$': 5,\n",
       " 'burger': 1985,\n",
       " 'yourself': 9177,\n",
       " 'move': 5513,\n",
       " 'pain': 6039,\n",
       " 'killing': 4754,\n",
       " 'good': 3836,\n",
       " 'joke': 4636,\n",
       " 'girls': 3785,\n",
       " 'situation': 7338,\n",
       " 'seekers': 7102,\n",
       " 'part': 6081,\n",
       " 'checking': 2208,\n",
       " 'iq': 4508,\n",
       " 'roommates': 6910,\n",
       " 'took': 8245,\n",
       " 'forever': 3557,\n",
       " 'come': 2371,\n",
       " 'double': 2966,\n",
       " 'check': 2204,\n",
       " 'hair': 3972,\n",
       " 'dresser': 2998,\n",
       " 'said': 6980,\n",
       " 'wun': 9024,\n",
       " 'cut': 2624,\n",
       " 'short': 7248,\n",
       " 'nice': 5701,\n",
       " 'pleased': 6266,\n",
       " 'advise': 1165,\n",
       " 'following': 3534,\n",
       " 'recent': 6692,\n",
       " 'review': 6849,\n",
       " 'mob': 5439,\n",
       " 'awarded': 1549,\n",
       " '1500': 414,\n",
       " 'bonus': 1844,\n",
       " '09066364589': 306,\n",
       " 'song': 7478,\n",
       " 'dedicated': 2722,\n",
       " 'which': 8853,\n",
       " 'dedicate': 2721,\n",
       " 'valuable': 8566,\n",
       " 'frnds': 3643,\n",
       " 'rply': 6925,\n",
       " 'complimentary': 2406,\n",
       " 'trip': 8322,\n",
       " 'eurodisinc': 3234,\n",
       " 'trav': 8304,\n",
       " 'aco': 1119,\n",
       " '41': 679,\n",
       " '1000': 356,\n",
       " 'dis': 2871,\n",
       " '6': 785,\n",
       " 'morefrmmob': 5490,\n",
       " 'shracomorsglsuplt': 7273,\n",
       " '10': 350,\n",
       " 'ls1': 5103,\n",
       " '3aj': 638,\n",
       " 'hear': 4062,\n",
       " 'divorce': 2900,\n",
       " 'barbie': 1620,\n",
       " 'comes': 2373,\n",
       " \"ken's\": 4728,\n",
       " 'plane': 6247,\n",
       " 'wah': 8682,\n",
       " 'lucky': 5114,\n",
       " 'save': 7029,\n",
       " 'money': 5470,\n",
       " 'hee': 4075,\n",
       " 'finished': 3464,\n",
       " 'hi': 4120,\n",
       " 'babe': 1574,\n",
       " 'im': 4368,\n",
       " 'wanna': 8713,\n",
       " 'something': 7464,\n",
       " 'xx': 9094,\n",
       " 'performed': 6155,\n",
       " 'waiting': 8692,\n",
       " 'machan': 5158,\n",
       " 'once': 5901,\n",
       " 'thats': 8051,\n",
       " 'cool': 2481,\n",
       " 'gentleman': 3751,\n",
       " 'dignity': 2848,\n",
       " 'respect': 6816,\n",
       " 'peoples': 6147,\n",
       " 'very': 8598,\n",
       " 'much': 5544,\n",
       " 'shy': 7283,\n",
       " 'pa': 6027,\n",
       " 'operate': 5928,\n",
       " 'after': 1183,\n",
       " 'same': 6996,\n",
       " 'looking': 5050,\n",
       " 'job': 4623,\n",
       " \"ta's\": 7896,\n",
       " 'earn': 3070,\n",
       " 'ah': 1204,\n",
       " 'stop': 7688,\n",
       " 'urgnt': 8517,\n",
       " 'real': 6662,\n",
       " 'yo': 9152,\n",
       " 'tickets': 8142,\n",
       " 'one': 5903,\n",
       " 'jacket': 4563,\n",
       " 'used': 8532,\n",
       " 'multis': 5553,\n",
       " 'started': 7632,\n",
       " 'requests': 6797,\n",
       " 'came': 2057,\n",
       " 'bed': 1686,\n",
       " 'coins': 2350,\n",
       " 'factory': 3335,\n",
       " 'gotta': 3860,\n",
       " 'nitros': 5727,\n",
       " 'ela': 3124,\n",
       " 'kano': 4708,\n",
       " 'il': 4362,\n",
       " 'download': 2975,\n",
       " 'wen': 8811,\n",
       " 'don': 2947,\n",
       " 'stand': 7620,\n",
       " 'close': 2313,\n",
       " 'll': 5008,\n",
       " 'another': 1332,\n",
       " 'night': 5710,\n",
       " 'spent': 7550,\n",
       " 'late': 4858,\n",
       " 'afternoon': 1185,\n",
       " 'casualty': 2126,\n",
       " 'means': 5291,\n",
       " \"haven't\": 4039,\n",
       " 'any': 1346,\n",
       " 'y': 9107,\n",
       " '42moro': 689,\n",
       " 'includes': 4405,\n",
       " 'sheets': 7203,\n",
       " 'smile': 7403,\n",
       " 'pleasure': 6268,\n",
       " 'trouble': 8328,\n",
       " 'pours': 6359,\n",
       " 'rain': 6602,\n",
       " 'sum': 7798,\n",
       " 'hurts': 4297,\n",
       " 'becoz': 1684,\n",
       " 'someone': 7457,\n",
       " 'loves': 5090,\n",
       " 'smiling': 7407,\n",
       " 'service': 7150,\n",
       " 'representative': 6794,\n",
       " '0800 169 6031': 86,\n",
       " 'between': 1741,\n",
       " '10am': 365,\n",
       " '9pm': 1002,\n",
       " 'guaranteed': 3930,\n",
       " '5000': 755,\n",
       " 'havent': 4040,\n",
       " 'planning': 6251,\n",
       " 'buy': 2004,\n",
       " 'lido': 4937,\n",
       " '530': 767,\n",
       " 'show': 7264,\n",
       " 'collected': 2358,\n",
       " 'simply': 7311,\n",
       " 'password': 6102,\n",
       " 'mix': 5421,\n",
       " '85069': 934,\n",
       " 'verify': 8594,\n",
       " 'usher': 8538,\n",
       " 'britney': 1932,\n",
       " 'fml': 3525,\n",
       " 'po': 6284,\n",
       " 'box': 1879,\n",
       " '5249': 764,\n",
       " 'mk17': 5424,\n",
       " '92h': 990,\n",
       " '450ppw': 705,\n",
       " 'telugu': 7991,\n",
       " 'movie': 5516,\n",
       " 'abt': 1084,\n",
       " 'loads': 5014,\n",
       " 'loans': 5016,\n",
       " 'wk': 8928,\n",
       " 'hols': 4174,\n",
       " 'run': 6946,\n",
       " 'forgot': 3565,\n",
       " 'hairdressers': 3974,\n",
       " 'appointment': 1386,\n",
       " 'four': 3584,\n",
       " 'shower': 7266,\n",
       " 'beforehand': 1702,\n",
       " 'cause': 2133,\n",
       " 'prob': 6456,\n",
       " 'coffee': 2345,\n",
       " 'animation': 1319,\n",
       " 'nothing': 5774,\n",
       " 'else': 3138,\n",
       " 'okay': 5877,\n",
       " 'price': 6431,\n",
       " 'long': 5042,\n",
       " 'legal': 4904,\n",
       " 'them': 8061,\n",
       " 'ave': 1536,\n",
       " 'ams': 1301,\n",
       " 'gone': 3832,\n",
       " '4the': 735,\n",
       " 'driving': 3007,\n",
       " 'test': 8014,\n",
       " 'yet': 9142,\n",
       " \"you're\": 9162,\n",
       " 'mean': 5287,\n",
       " 'guess': 3936,\n",
       " 'gave': 3729,\n",
       " 'boston': 1866,\n",
       " 'men': 5326,\n",
       " 'changed': 2174,\n",
       " 'search': 7080,\n",
       " 'location': 5019,\n",
       " 'nyc': 5819,\n",
       " 'cuz': 2631,\n",
       " 'signin': 7299,\n",
       " 'page': 6035,\n",
       " 'says': 7038,\n",
       " 'umma': 8423,\n",
       " 'life': 4940,\n",
       " 'vava': 8580,\n",
       " 'lot': 5066,\n",
       " 'dear': 2699,\n",
       " 'wishes': 8912,\n",
       " 'birthday': 1777,\n",
       " 'making': 5197,\n",
       " 'truly': 8335,\n",
       " 'memorable': 5323,\n",
       " 'aight': 1216,\n",
       " 'hit': 4141,\n",
       " 'would': 8993,\n",
       " 'ip': 4501,\n",
       " 'address': 1141,\n",
       " 'considering': 2449,\n",
       " 'computer': 2412,\n",
       " \"isn't\": 4528,\n",
       " 'minecraft': 5380,\n",
       " 'server': 7149,\n",
       " 'grumpy': 3923,\n",
       " 'old': 5889,\n",
       " 'people': 6146,\n",
       " 'mom': 5462,\n",
       " 'better': 1738,\n",
       " 'lying': 5135,\n",
       " 'jokes': 4640,\n",
       " 'worry': 8983,\n",
       " 'busy': 1998,\n",
       " 'plural': 6277,\n",
       " 'noun': 5781,\n",
       " 'research': 6802,\n",
       " 'dinner.msg': 2859,\n",
       " 'cos': 2499,\n",
       " 'things': 8091,\n",
       " 'scared': 7044,\n",
       " 'mah': 5180,\n",
       " 'loud': 5076,\n",
       " 'gent': 3749,\n",
       " 'contact': 2454,\n",
       " 'last': 4855,\n",
       " 'weekends': 8793,\n",
       " 'draw': 2989,\n",
       " 'shows': 7272,\n",
       " '09064012160': 282,\n",
       " 'k52': 4691,\n",
       " '12hrs': 398,\n",
       " '150ppm': 419,\n",
       " 'wa': 8677,\n",
       " 'openin': 5925,\n",
       " 'sentence': 7138,\n",
       " 'formal': 3569,\n",
       " 'anyway': 1360,\n",
       " 'juz': 4682,\n",
       " 'tt': 8348,\n",
       " 'eatin': 3083,\n",
       " 'puttin': 6554,\n",
       " 'weight': 8798,\n",
       " 'haha': 3968,\n",
       " 'anythin': 1355,\n",
       " 'happened': 4003,\n",
       " 'entered': 3185,\n",
       " 'cabin': 2025,\n",
       " \"b'day\": 1562,\n",
       " 'boss': 1865,\n",
       " 'felt': 3411,\n",
       " 'askd': 1464,\n",
       " 'invited': 4494,\n",
       " 'apartment': 1365,\n",
       " 'went': 8814,\n",
       " 'specially': 7536,\n",
       " 'holiday': 4171,\n",
       " 'flights': 3502,\n",
       " 'inc': 4399,\n",
       " 'operator': 5929,\n",
       " '08712778109': 166,\n",
       " '10p': 368,\n",
       " 'min': 5372,\n",
       " 'goodo': 3846,\n",
       " 'must': 5575,\n",
       " 'friday': 3626,\n",
       " 'egg-potato': 3112,\n",
       " 'ratio': 6631,\n",
       " 'tortilla': 8262,\n",
       " 'needed': 5656,\n",
       " 'hmm': 4153,\n",
       " 'uncle': 8433,\n",
       " 'informed': 4438,\n",
       " 'paying': 6124,\n",
       " 'school': 7050,\n",
       " 'directly': 2865,\n",
       " 'food': 3542,\n",
       " 'private': 6447,\n",
       " '2004': 486,\n",
       " 'account': 1107,\n",
       " 'statement': 7641,\n",
       " '07742676969': 64,\n",
       " '786': 864,\n",
       " 'unredeemed': 8477,\n",
       " 'points': 6297,\n",
       " '08719180248': 213,\n",
       " 'identifier': 4344,\n",
       " '45239': 707,\n",
       " 'expires': 3307,\n",
       " '2000': 484,\n",
       " 'caller': 2045,\n",
       " '5/9': 752,\n",
       " '03': 46,\n",
       " 'landline': 4835,\n",
       " '09064019788': 288,\n",
       " '42wr29c': 690,\n",
       " 'apples': 1381,\n",
       " 'pairs': 6044,\n",
       " 'malarky': 5199,\n",
       " 'todays': 8205,\n",
       " 'voda': 8645,\n",
       " 'numbers': 5804,\n",
       " 'ending': 3160,\n",
       " '7548': 856,\n",
       " '350': 624,\n",
       " 'award': 1548,\n",
       " 'match': 5251,\n",
       " '08712300220': 149,\n",
       " 'quoting': 6589,\n",
       " '4041': 674,\n",
       " 'standard': 7621,\n",
       " 'rates': 6629,\n",
       " 'app': 1375,\n",
       " 'sao': 7004,\n",
       " 'mu': 5542,\n",
       " 'predict': 6392,\n",
       " \"ü'll\": 9222,\n",
       " 'buying': 2007,\n",
       " 'yetunde': 9144,\n",
       " \"hasn't\": 4024,\n",
       " 'sent': 7137,\n",
       " 'bother': 1869,\n",
       " 'sending': 7129,\n",
       " 'involve': 4498,\n",
       " \"shouldn't\": 7259,\n",
       " 'imposed': 4386,\n",
       " 'apologise': 1372,\n",
       " 'girl': 3782,\n",
       " 'del': 2740,\n",
       " 'bak': 1597,\n",
       " 'lucyxx': 5118,\n",
       " 'tmorrow.pls': 8185,\n",
       " 'accomodate': 1102,\n",
       " 'answer': 1335,\n",
       " 'sunshine': 7812,\n",
       " 'quiz': 6584,\n",
       " 'q': 6559,\n",
       " 'top': 8253,\n",
       " 'sony': 7480,\n",
       " 'dvd': 3051,\n",
       " 'player': 6257,\n",
       " 'country': 2518,\n",
       " 'algarve': 1245,\n",
       " 'ansr': 1334,\n",
       " '82277': 907,\n",
       " 'sp': 7516,\n",
       " 'tyrone': 8394,\n",
       " 'laid': 4827,\n",
       " 'dogging': 2932,\n",
       " 'locations': 5020,\n",
       " 'direct': 2864,\n",
       " 'join': 4631,\n",
       " \"uk's\": 8416,\n",
       " 'largest': 4852,\n",
       " 'bt': 1958,\n",
       " 'txting': 8383,\n",
       " 'gravel': 3888,\n",
       " '69888': 822,\n",
       " 'nt': 5791,\n",
       " 'ec2a': 3086,\n",
       " '31p': 611,\n",
       " '@150p': 1039,\n",
       " 'haf': 3967,\n",
       " 'msn': 5534,\n",
       " 'yijue@hotmail.com': 9149,\n",
       " 'him': 4132,\n",
       " 'rooms': 6911,\n",
       " 'befor': 1699,\n",
       " 'activities': 1129,\n",
       " \"you'll\": 9161,\n",
       " 'msgs': 5533,\n",
       " 'chat': 2195,\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_nums, terms = zip(*sorted(zip(tfidf.vocabulary_.values(),tfidf.vocabulary_.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('!',\n",
       " '\"',\n",
       " '#',\n",
       " '#150',\n",
       " '#5000',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '. .',\n",
       " '. . .',\n",
       " '. . . .',\n",
       " '. . . . .',\n",
       " '. ..',\n",
       " '..',\n",
       " '.. .',\n",
       " '.. . . .',\n",
       " '.. ... ...',\n",
       " '...',\n",
       " '... . . . .',\n",
       " '/',\n",
       " '0',\n",
       " '00',\n",
       " '00870405040',\n",
       " '0089',\n",
       " '01',\n",
       " '0121 2025050',\n",
       " '01223585236',\n",
       " '01223585334',\n",
       " '01256987',\n",
       " '02',\n",
       " '02/06',\n",
       " '02/09',\n",
       " '0207 153 9153',\n",
       " '0207 153 9996',\n",
       " '0207-083-6089',\n",
       " '02072069400',\n",
       " '02073162414',\n",
       " '02085076972',\n",
       " '03',\n",
       " '03530150',\n",
       " '04',\n",
       " '04/09',\n",
       " '05',\n",
       " '050703',\n",
       " '06',\n",
       " '06.05',\n",
       " '06/11',\n",
       " '07/11',\n",
       " '07008009200',\n",
       " '07046744435',\n",
       " '07090201529',\n",
       " '07090298926',\n",
       " '07099833605',\n",
       " '07123456789',\n",
       " '07732584351',\n",
       " '07734396839',\n",
       " '07742676969',\n",
       " '07753741225',\n",
       " '0776xxxxxxx',\n",
       " '07786200117',\n",
       " '077xxx',\n",
       " '078',\n",
       " '07801543489',\n",
       " '07808',\n",
       " '07808247860',\n",
       " '07808726822',\n",
       " '07815296484',\n",
       " '07821230901',\n",
       " '078498',\n",
       " '07880867867',\n",
       " '0789xxxxxxx',\n",
       " '07946746291',\n",
       " '0796xxxxxx',\n",
       " '07973788240',\n",
       " '07xxxxxxxxx',\n",
       " '08',\n",
       " '0800',\n",
       " '0800 0721072',\n",
       " '0800 169 6031',\n",
       " '0800 195 6669',\n",
       " '0800 1956669',\n",
       " '0800 5050',\n",
       " '0800 542 0578',\n",
       " '0800 542 0825',\n",
       " '08000407165',\n",
       " '08000776320',\n",
       " '08000839402',\n",
       " '08000930705',\n",
       " '08000938767',\n",
       " '08001950382',\n",
       " '08002888812',\n",
       " '08002986030',\n",
       " '08002986906',\n",
       " '08002988890',\n",
       " '08006344447',\n",
       " '0808 145 4742',\n",
       " '08081263000',\n",
       " '08081560665',\n",
       " '0819',\n",
       " '0844',\n",
       " '08448350055',\n",
       " '08448714184',\n",
       " '0845 021 3680',\n",
       " '0845 2814032',\n",
       " '08450542832',\n",
       " '08452810071',\n",
       " '08452810073',\n",
       " '08452810075',\n",
       " '0870',\n",
       " '08700435505',\n",
       " '08700469649',\n",
       " '08700621170',\n",
       " '08701213186',\n",
       " '08701237397',\n",
       " '08701417012',\n",
       " '08701624',\n",
       " '08701752560',\n",
       " '08701872873',\n",
       " '08702411827',\n",
       " '08702490080',\n",
       " '08702840625',\n",
       " '08702840625.comuk',\n",
       " '08704050406',\n",
       " '08704439680',\n",
       " '08706091795',\n",
       " '08707379102',\n",
       " '08707500020',\n",
       " '08707509020',\n",
       " '08707533310',\n",
       " '08707808226',\n",
       " '08708034412',\n",
       " '08708800282',\n",
       " '08709222922',\n",
       " '08709501522',\n",
       " '0871-4719',\n",
       " '0871-872-9755',\n",
       " '0871-872-9758',\n",
       " '08710471114',\n",
       " '08712101358',\n",
       " '08712103738',\n",
       " '08712120250',\n",
       " '08712300220',\n",
       " '08712317606',\n",
       " '08712400200',\n",
       " '08712400602',\n",
       " '08712400603',\n",
       " '08712402050',\n",
       " '08712402578',\n",
       " '08712402779',\n",
       " '08712402902',\n",
       " '08712402972',\n",
       " '08712404000',\n",
       " '08712405020',\n",
       " '08712405022',\n",
       " '08712460324',\n",
       " '08712466669',\n",
       " '08712778107',\n",
       " '08712778108',\n",
       " '08712778109',\n",
       " '08714342399',\n",
       " '08714712377',\n",
       " '08714712379',\n",
       " '08714712388',\n",
       " '08714712394',\n",
       " '08714712412',\n",
       " '08714714011',\n",
       " '08714740323',\n",
       " '08714742804',\n",
       " '08715203028',\n",
       " '08715203649',\n",
       " '08715203652',\n",
       " '08715203656',\n",
       " '08715203677',\n",
       " '08715203685',\n",
       " '08715203694',\n",
       " '08715205273',\n",
       " '08715500022',\n",
       " '08715705022',\n",
       " '08717111821',\n",
       " '08717168528',\n",
       " '08717205546',\n",
       " '0871750',\n",
       " '08717507382',\n",
       " '08717509990',\n",
       " '08717890890',\n",
       " '08717895698',\n",
       " '08717898035',\n",
       " '08718711108',\n",
       " '08718720201',\n",
       " '08718723815',\n",
       " '08718725756',\n",
       " '08718726270',\n",
       " '08718726970',\n",
       " '08718726971',\n",
       " '08718726978',\n",
       " '08718727200',\n",
       " '08718727868',\n",
       " '08718727870',\n",
       " '08718728876',\n",
       " '08718730555',\n",
       " '08718730666',\n",
       " '08718738001',\n",
       " '08718738002',\n",
       " '08718738034',\n",
       " '08719180219',\n",
       " '08719180248',\n",
       " '08719181259',\n",
       " '08719181503',\n",
       " '08719181513',\n",
       " '08719839835',\n",
       " '08719899217',\n",
       " '08719899229',\n",
       " '08719899230',\n",
       " '09',\n",
       " '09041940223',\n",
       " '09050000301',\n",
       " '09050000327',\n",
       " '09050000332',\n",
       " '09050000460',\n",
       " '09050000555',\n",
       " '09050000878',\n",
       " '09050000928',\n",
       " '09050001295',\n",
       " '09050001808',\n",
       " '09050002311',\n",
       " '09050003091',\n",
       " '09050005321',\n",
       " '09050090044',\n",
       " '09050280520',\n",
       " '09053750005',\n",
       " '09056242159',\n",
       " '09057039994',\n",
       " '09058091854',\n",
       " '09058091870',\n",
       " '09058094454',\n",
       " '09058094455',\n",
       " '09058094507',\n",
       " '09058094565',\n",
       " '09058094583',\n",
       " '09058094594',\n",
       " '09058094597',\n",
       " '09058094599',\n",
       " '09058095107',\n",
       " '09058095201',\n",
       " '09058097189',\n",
       " '09058097218',\n",
       " '09058098002',\n",
       " '09058099801',\n",
       " '09061104276',\n",
       " '09061104283',\n",
       " '09061209465',\n",
       " '09061213237',\n",
       " '09061221061',\n",
       " '09061221066',\n",
       " '09061701444',\n",
       " '09061701461',\n",
       " '09061701851',\n",
       " '09061701939',\n",
       " '09061702893',\n",
       " '09061743386',\n",
       " '09061743806',\n",
       " '09061743810',\n",
       " '09061743811',\n",
       " '09061744553',\n",
       " '09061749602',\n",
       " '09061790121',\n",
       " '09061790125',\n",
       " '09061790126',\n",
       " '09063440451',\n",
       " '09063442151',\n",
       " '09063458130',\n",
       " '09063463',\n",
       " '09064011000',\n",
       " '09064012103',\n",
       " '09064012160',\n",
       " '09064015307',\n",
       " '09064017295',\n",
       " '09064017305',\n",
       " '09064018838',\n",
       " '09064019014',\n",
       " '09064019788',\n",
       " '09065069120',\n",
       " '09065069154',\n",
       " '09065171142',\n",
       " '09065174042',\n",
       " '09065394514',\n",
       " '09065394973',\n",
       " '09065989180',\n",
       " '09065989182',\n",
       " '09066350750',\n",
       " '09066358152',\n",
       " '09066358361',\n",
       " '09066361921',\n",
       " '09066362206',\n",
       " '09066362220',\n",
       " '09066362231',\n",
       " '09066364311',\n",
       " '09066364349',\n",
       " '09066364589',\n",
       " '09066368327',\n",
       " '09066368470',\n",
       " '09066368753',\n",
       " '09066380611',\n",
       " '09066382422',\n",
       " '09066612661',\n",
       " '09066649731',\n",
       " '09066660100',\n",
       " '09071512432',\n",
       " '09071512433',\n",
       " '09071517866',\n",
       " '09077818151',\n",
       " '09090204448',\n",
       " '09090900040',\n",
       " '09094100151',\n",
       " '09094646631',\n",
       " '09094646899',\n",
       " '09095350301',\n",
       " '09096102316',\n",
       " '09099725823',\n",
       " '09099726395',\n",
       " '09099726429',\n",
       " '09099726481',\n",
       " '09099726553',\n",
       " '09111030116',\n",
       " '09111032124',\n",
       " '09701213186',\n",
       " '0a',\n",
       " '0p',\n",
       " '0quit',\n",
       " '1',\n",
       " '1,000',\n",
       " '1,2',\n",
       " '1,50',\n",
       " '1,500',\n",
       " '1.20',\n",
       " '1.5',\n",
       " '1.50',\n",
       " '1.childish',\n",
       " '1/08',\n",
       " '1/1',\n",
       " '1/2',\n",
       " '1/3',\n",
       " '10',\n",
       " '10,000',\n",
       " '10.1',\n",
       " '10/06',\n",
       " '100',\n",
       " '100,000',\n",
       " '1000',\n",
       " '1000call',\n",
       " '1000s',\n",
       " '100p',\n",
       " '100txt',\n",
       " '1013',\n",
       " '1030',\n",
       " '10:10',\n",
       " '10:30',\n",
       " '10am',\n",
       " '10k',\n",
       " '10mins',\n",
       " '10p',\n",
       " '10ppm',\n",
       " '10th',\n",
       " '11',\n",
       " '11.48',\n",
       " '1120',\n",
       " '113',\n",
       " '1131',\n",
       " '114/14',\n",
       " '1146',\n",
       " '1151',\n",
       " '116',\n",
       " '1172',\n",
       " '118p',\n",
       " '11mths',\n",
       " '11pm',\n",
       " '12',\n",
       " '12,000',\n",
       " '1205',\n",
       " '120p',\n",
       " '121',\n",
       " '1225',\n",
       " '123',\n",
       " '125',\n",
       " '1250',\n",
       " '125gift',\n",
       " '128',\n",
       " '1282essexcm61xn',\n",
       " '12:30',\n",
       " '12hours',\n",
       " '12hrs',\n",
       " '12mths',\n",
       " '12n146tf15',\n",
       " '12n146tf150p',\n",
       " '13/10',\n",
       " '13/4',\n",
       " '130',\n",
       " '1327',\n",
       " '139',\n",
       " '140',\n",
       " '1405',\n",
       " '140ppm',\n",
       " '1450',\n",
       " '146tf150p',\n",
       " '14thmarch',\n",
       " '150',\n",
       " '1500',\n",
       " '150p',\n",
       " '150p16',\n",
       " '150pm',\n",
       " '150ppermesssubscription',\n",
       " '150ppm',\n",
       " '150ppmmobilesvary',\n",
       " '150ppmpobox10183bhamb64xe',\n",
       " '150ppmsg',\n",
       " '150ppmx3age16',\n",
       " '150pw',\n",
       " '150x3',\n",
       " '151',\n",
       " '1510',\n",
       " '15541',\n",
       " '15:26',\n",
       " '15h',\n",
       " '16',\n",
       " '16.150',\n",
       " '165',\n",
       " '1680',\n",
       " '16yrs',\n",
       " '177',\n",
       " '177hp51fl',\n",
       " '18',\n",
       " '18/11',\n",
       " '180',\n",
       " '1843',\n",
       " '1896wc1n3xx',\n",
       " '18:0430-',\n",
       " '18p',\n",
       " '18yrs',\n",
       " '1apple',\n",
       " '1b6a5ecef91ff9',\n",
       " '1cup',\n",
       " '1da',\n",
       " '1er',\n",
       " '1hr',\n",
       " '1im',\n",
       " '1lemon',\n",
       " '1million',\n",
       " '1more',\n",
       " '1n3xx',\n",
       " '1pm',\n",
       " '1st',\n",
       " '1st4terms',\n",
       " '1stchoice.co.uk',\n",
       " '1stone',\n",
       " '1thing',\n",
       " '1tulsi',\n",
       " '1win150ppmx3',\n",
       " '1win150ppmx3age16',\n",
       " '1win150ppmx3age16subscription',\n",
       " '1winaweek',\n",
       " '1winawk',\n",
       " '1x150p',\n",
       " '1yf',\n",
       " '2',\n",
       " '2,000',\n",
       " '2-4-',\n",
       " '2.15',\n",
       " '2.30',\n",
       " '2.50',\n",
       " '2.im',\n",
       " '2.naughty',\n",
       " '2/2',\n",
       " '2/3',\n",
       " '20',\n",
       " '20,000',\n",
       " '200',\n",
       " '2000',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '200p',\n",
       " '202',\n",
       " '20m12aq',\n",
       " '20p',\n",
       " '21',\n",
       " '21/11',\n",
       " '2187000',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '220cm2',\n",
       " '23',\n",
       " '2309',\n",
       " '23f',\n",
       " '23g',\n",
       " '24',\n",
       " '24/10',\n",
       " '24/7',\n",
       " '245c2150pm',\n",
       " '24hrs',\n",
       " '24m',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '250k',\n",
       " '255',\n",
       " '25p',\n",
       " '26.03',\n",
       " '26/10',\n",
       " '26/11',\n",
       " '2667',\n",
       " '26th',\n",
       " '27/03',\n",
       " '27/6',\n",
       " '28',\n",
       " '28/5',\n",
       " '28days',\n",
       " '28th',\n",
       " '28thfeb',\n",
       " '29',\n",
       " '29/03',\n",
       " '29/10',\n",
       " '2b',\n",
       " '2bed',\n",
       " '2bold',\n",
       " '2bremoved',\n",
       " '2c',\n",
       " '2channel',\n",
       " '2come',\n",
       " '2day',\n",
       " '2day.love',\n",
       " '2die',\n",
       " '2docd.please',\n",
       " '2end',\n",
       " '2exit',\n",
       " '2ez',\n",
       " '2find',\n",
       " '2getha',\n",
       " '2geva',\n",
       " '2go',\n",
       " '2go.did',\n",
       " '2gthr',\n",
       " '2hear',\n",
       " '2hook',\n",
       " '2hrs',\n",
       " '2i',\n",
       " '2kbsubject',\n",
       " '2marrow',\n",
       " '2mobile',\n",
       " '2moro',\n",
       " '2morow',\n",
       " '2morro',\n",
       " '2morrow',\n",
       " '2morrowxxxx',\n",
       " '2mro',\n",
       " '2mrw',\n",
       " '2mwen',\n",
       " '2nd',\n",
       " '2nhite',\n",
       " '2nights',\n",
       " '2nite',\n",
       " '2optout',\n",
       " '2p',\n",
       " '2px',\n",
       " '2rcv',\n",
       " '2stop',\n",
       " '2stoptx',\n",
       " '2stoptxt',\n",
       " '2tell',\n",
       " '2the',\n",
       " '2u',\n",
       " '2u2',\n",
       " '2watershd',\n",
       " '2waxsto',\n",
       " '2wks',\n",
       " '2worzels',\n",
       " '2wt',\n",
       " '2wu',\n",
       " '2years',\n",
       " '2yr',\n",
       " '2yrs',\n",
       " '3',\n",
       " '3.00',\n",
       " '3.75',\n",
       " '3.99',\n",
       " '3.sentiment',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '300603',\n",
       " '300603t',\n",
       " '300p',\n",
       " '3030',\n",
       " '30apr',\n",
       " '30pp',\n",
       " '30s',\n",
       " '30th',\n",
       " '31',\n",
       " '31/10',\n",
       " '3100',\n",
       " '310303',\n",
       " '31p',\n",
       " '32',\n",
       " '32000',\n",
       " '3230',\n",
       " '32323',\n",
       " '326',\n",
       " '33.65',\n",
       " '330',\n",
       " '334',\n",
       " '334sk38ch',\n",
       " '3355',\n",
       " '33:50',\n",
       " '342/2',\n",
       " '350',\n",
       " '3510i',\n",
       " '35p',\n",
       " '3650',\n",
       " '36504',\n",
       " '36504w45wq',\n",
       " '365o4w45wq',\n",
       " '373',\n",
       " '3750',\n",
       " '37819',\n",
       " '38',\n",
       " '385',\n",
       " '391784',\n",
       " '39822',\n",
       " '3aj',\n",
       " '3cktz8r7',\n",
       " '3d',\n",
       " '3days',\n",
       " '3g',\n",
       " '3gbp',\n",
       " '3hrs',\n",
       " '3lions',\n",
       " '3lp',\n",
       " '3miles',\n",
       " '3mins',\n",
       " '3mobile',\n",
       " '3optical',\n",
       " '3pound',\n",
       " '3qxj9',\n",
       " '3rd',\n",
       " '3ss',\n",
       " '3uz',\n",
       " '3wks',\n",
       " '3x',\n",
       " '3xx',\n",
       " '4',\n",
       " '4-6',\n",
       " '4-7',\n",
       " '4.15',\n",
       " '4.30',\n",
       " '4.47',\n",
       " '4.49',\n",
       " '4.50',\n",
       " '4.cook',\n",
       " '4.rowdy',\n",
       " '40',\n",
       " '400',\n",
       " '400mins',\n",
       " '402',\n",
       " '403',\n",
       " '4041',\n",
       " '40411',\n",
       " '40533',\n",
       " '40gb',\n",
       " '40mph',\n",
       " '41',\n",
       " '41685',\n",
       " '41782',\n",
       " '420',\n",
       " '42049',\n",
       " '4217',\n",
       " '4235wc1n3xx',\n",
       " '42478',\n",
       " '42810',\n",
       " '4284',\n",
       " '42moro',\n",
       " '42wr29c',\n",
       " '430',\n",
       " '434',\n",
       " '434sk38wp150ppm18',\n",
       " '44',\n",
       " '440',\n",
       " '4403ldnw1a7rw18',\n",
       " '4477977060',\n",
       " '4478012592',\n",
       " '4487124040',\n",
       " '4490500003',\n",
       " '4490715124',\n",
       " '45',\n",
       " '450',\n",
       " '450p',\n",
       " '450ppw',\n",
       " '450pw',\n",
       " '45239',\n",
       " '45po139wa',\n",
       " '45w2tg150p',\n",
       " '47',\n",
       " '48',\n",
       " '4882',\n",
       " '48922',\n",
       " '49557',\n",
       " '4a',\n",
       " '4an18th',\n",
       " '4brekkie',\n",
       " '4d',\n",
       " '4eva',\n",
       " '4few',\n",
       " '4fil',\n",
       " '4get',\n",
       " '4get2text',\n",
       " '4give',\n",
       " '4got',\n",
       " '4goten',\n",
       " '4info',\n",
       " '4jx',\n",
       " '4msgs',\n",
       " '4mths',\n",
       " '4my',\n",
       " '4qf2',\n",
       " '4t',\n",
       " '4th',\n",
       " '4the',\n",
       " '4thnov.behind',\n",
       " '4txt',\n",
       " '4u',\n",
       " '4utxt',\n",
       " '4w',\n",
       " '4ward',\n",
       " '4wrd',\n",
       " '4xx26',\n",
       " '4years',\n",
       " '5',\n",
       " '5.00',\n",
       " '5.15',\n",
       " '5.30',\n",
       " '5.ful',\n",
       " '5.gardener',\n",
       " '5.terror',\n",
       " '5/9',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '5000.00',\n",
       " '50award',\n",
       " '50p',\n",
       " '50s',\n",
       " '5120',\n",
       " '515',\n",
       " '5226',\n",
       " '523',\n",
       " '5249',\n",
       " '526',\n",
       " '528',\n",
       " '530',\n",
       " '54',\n",
       " '545',\n",
       " '5digital',\n",
       " '5free',\n",
       " '5ish',\n",
       " '5k',\n",
       " '5min',\n",
       " '5mls',\n",
       " '5p',\n",
       " '5pm',\n",
       " '5th',\n",
       " '5times',\n",
       " '5wb',\n",
       " '5we',\n",
       " '5wkg',\n",
       " '5wq',\n",
       " '5years',\n",
       " '6',\n",
       " '6.30',\n",
       " '6.45',\n",
       " '6.cruel',\n",
       " '6.house',\n",
       " '6.romantic',\n",
       " '60',\n",
       " '60,400',\n",
       " '600',\n",
       " '60p',\n",
       " '61',\n",
       " '61200',\n",
       " '61610',\n",
       " '62220cncl',\n",
       " '6230',\n",
       " '62468',\n",
       " '62735',\n",
       " '630',\n",
       " '63miles',\n",
       " '645',\n",
       " '65,61',\n",
       " '650',\n",
       " '66,382',\n",
       " '6600',\n",
       " '6650',\n",
       " '674',\n",
       " '6744123',\n",
       " '68866',\n",
       " '69',\n",
       " '69101',\n",
       " '69200',\n",
       " '69669',\n",
       " '69696',\n",
       " '69698',\n",
       " '69855',\n",
       " '69866.18',\n",
       " '69876',\n",
       " '69888',\n",
       " '69888nyt',\n",
       " '69911',\n",
       " '69969',\n",
       " '69988',\n",
       " '6days',\n",
       " '6gbp',\n",
       " '6hl',\n",
       " '6hrs',\n",
       " '6ish',\n",
       " '6missed',\n",
       " '6months',\n",
       " '6ph',\n",
       " '6pm',\n",
       " '6th',\n",
       " '6times',\n",
       " '6wu',\n",
       " '6zf',\n",
       " '7',\n",
       " '7.30',\n",
       " '7.8',\n",
       " '7.children',\n",
       " '7.romantic',\n",
       " '7.shy',\n",
       " '700',\n",
       " '71',\n",
       " '7250',\n",
       " '7250i',\n",
       " '730',\n",
       " '731',\n",
       " '734ls27yf',\n",
       " '74355',\n",
       " '75,000',\n",
       " '750',\n",
       " '7548',\n",
       " '75ldns7',\n",
       " '762',\n",
       " '7634',\n",
       " '7684',\n",
       " '77.11',\n",
       " '7732584351',\n",
       " '78',\n",
       " '786',\n",
       " '7876150',\n",
       " '79',\n",
       " '7:30',\n",
       " '7am',\n",
       " '7cfca1a',\n",
       " '7ish',\n",
       " '7oz',\n",
       " '7pm',\n",
       " '7th',\n",
       " '7ws',\n",
       " '7zs',\n",
       " '8',\n",
       " '8,22',\n",
       " '8-8',\n",
       " '8.30',\n",
       " '8.attractive',\n",
       " '8.lovable',\n",
       " '8.neighbour',\n",
       " '80',\n",
       " '800',\n",
       " '8000930705',\n",
       " '80062',\n",
       " '8007',\n",
       " '80082',\n",
       " '80086',\n",
       " '8012230',\n",
       " '80155',\n",
       " '80160',\n",
       " '80182',\n",
       " '8027',\n",
       " '80488',\n",
       " '80488.biz',\n",
       " '80608',\n",
       " '8077',\n",
       " '80878',\n",
       " '81010',\n",
       " '81151',\n",
       " '81303',\n",
       " '81618',\n",
       " '820554ad0a1705572711',\n",
       " '82228',\n",
       " '82242',\n",
       " '82277',\n",
       " '82277.unsub',\n",
       " '82324',\n",
       " '82468',\n",
       " '83021',\n",
       " '83039',\n",
       " '83049',\n",
       " '83110',\n",
       " '83118',\n",
       " '83222',\n",
       " '83332.please',\n",
       " '83338',\n",
       " '83355',\n",
       " '83370',\n",
       " '83383',\n",
       " '83435',\n",
       " '83600',\n",
       " '83738',\n",
       " '84',\n",
       " '84025',\n",
       " '84122',\n",
       " '84128',\n",
       " '84199',\n",
       " '84484',\n",
       " '85',\n",
       " '850',\n",
       " '85023',\n",
       " '85069',\n",
       " '85222',\n",
       " '85233',\n",
       " '8552',\n",
       " '85555',\n",
       " '86021',\n",
       " '861',\n",
       " '864233',\n",
       " '86688',\n",
       " '86888',\n",
       " '87021',\n",
       " '87066',\n",
       " '87070',\n",
       " '87077',\n",
       " '87121',\n",
       " '87131',\n",
       " '8714714',\n",
       " '87239',\n",
       " '87575',\n",
       " '8800',\n",
       " '88039',\n",
       " '88039.skilgme',\n",
       " '88066',\n",
       " '88088',\n",
       " '88222',\n",
       " '88600',\n",
       " '88800',\n",
       " '8883',\n",
       " '88877',\n",
       " '88888',\n",
       " '89',\n",
       " '89034',\n",
       " '89070',\n",
       " '89080',\n",
       " '89105',\n",
       " '89123',\n",
       " '89545',\n",
       " '89555',\n",
       " '89693',\n",
       " '89938',\n",
       " '8am',\n",
       " '8ball',\n",
       " '8i',\n",
       " '8lb',\n",
       " '8p',\n",
       " '8r',\n",
       " '8th',\n",
       " '8wp',\n",
       " '9',\n",
       " '9-6',\n",
       " '9.decent',\n",
       " '9.funny',\n",
       " '900',\n",
       " '9061100010',\n",
       " '910',\n",
       " '9280114',\n",
       " '92h',\n",
       " '930',\n",
       " '9307622',\n",
       " '945',\n",
       " '946',\n",
       " '95',\n",
       " '95qu',\n",
       " '97n7qp',\n",
       " '9832156',\n",
       " '9ae',\n",
       " ...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>#150</th>\n",
       "      <th>...</th>\n",
       "      <th>…</th>\n",
       "      <th>┾</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic0</th>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>0.064</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic2</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 9232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            !      \"      #   #150  ...      …      ┾    〨ud      鈥\n",
       "topic0 -0.071  0.008 -0.001 -0.000  ... -0.002  0.001  0.001  0.001\n",
       "topic1  0.064  0.008  0.000 -0.000  ...  0.003  0.001  0.001  0.001\n",
       "topic2  0.071  0.027  0.000  0.001  ...  0.002 -0.001 -0.001 -0.001\n",
       "topic3 -0.059 -0.032 -0.001 -0.000  ...  0.001  0.001  0.001  0.001\n",
       "\n",
       "[4 rows x 9232 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = pd.DataFrame(pca.components_, columns=terms,\n",
    "                       index=['topic{}'.format(i) for i in range(16)])\n",
    "pd.options.display.max_columns = 8\n",
    "weights.head(4).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>;)</th>\n",
       "      <th>:)</th>\n",
       "      <th>half</th>\n",
       "      <th>off</th>\n",
       "      <th>free</th>\n",
       "      <th>crazy</th>\n",
       "      <th>deal</th>\n",
       "      <th>only</th>\n",
       "      <th>$</th>\n",
       "      <th>80</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic0</th>\n",
       "      <td>-7.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic2</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>-5.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic4</th>\n",
       "      <td>38.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-12.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic5</th>\n",
       "      <td>-26.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic6</th>\n",
       "      <td>-11.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>19.8</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic7</th>\n",
       "      <td>16.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-17.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic8</th>\n",
       "      <td>34.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic9</th>\n",
       "      <td>7.6</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>16.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>6.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic10</th>\n",
       "      <td>-32.9</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic11</th>\n",
       "      <td>-15.8</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-30.1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic12</th>\n",
       "      <td>-21.8</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>42.8</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic13</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>27.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic14</th>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic15</th>\n",
       "      <td>-7.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            !   ;)    :)  half  off  free  crazy  deal  only    $   80    %\n",
       "topic0   -7.1  0.1  -0.5  -0.0 -0.4  -2.0   -0.0  -0.1  -2.2  0.3 -0.0 -0.0\n",
       "topic1    6.4  0.0   7.4   0.1  0.4  -2.3   -0.2  -0.1  -3.8 -0.1 -0.0 -0.2\n",
       "topic2    7.1  0.2  -0.1   0.0  0.3   4.4    0.1  -0.1   0.7  0.0  0.0  0.1\n",
       "topic3   -5.9 -0.3  -7.1   0.2  0.3  -0.2    0.0   0.1  -2.3  0.1 -0.1 -0.3\n",
       "topic4   38.2 -0.1 -12.4  -0.1 -0.2   9.9    0.1  -0.2   3.0  0.3  0.1 -0.1\n",
       "topic5  -26.5  0.1  -1.5  -0.3 -0.7  -1.4   -0.6  -0.2  -1.8 -0.9  0.0  0.0\n",
       "topic6  -11.0 -0.5  19.8  -0.4 -0.9  -0.5   -0.2  -0.2  -1.4 -0.0 -0.0 -0.1\n",
       "topic7   16.1  0.1 -17.7   0.7  0.8  -2.8    0.0   0.0  -1.8 -0.3  0.0 -0.1\n",
       "topic8   34.6  0.1   5.0  -0.4 -0.5   0.0   -0.4  -0.4   3.3 -0.6 -0.0 -0.2\n",
       "topic9    7.6 -0.3  16.6   1.5 -0.9   6.4   -0.5  -0.4   3.0 -0.5 -0.0  0.0\n",
       "topic10 -32.9 -0.2 -10.7   0.1  0.1  12.5    0.1  -0.0   0.3  0.0 -0.0 -0.2\n",
       "topic11 -15.8 -0.4 -30.1  -0.5 -1.5   4.9   -0.1  -0.0  -0.7  0.3  0.0  0.3\n",
       "topic12 -21.8 -0.3  42.8  -0.3  0.1  -1.6   -0.4   0.2   3.2  0.2 -0.0  0.3\n",
       "topic13  18.1 -0.2  27.2  -0.3  0.6   7.0    0.6   0.1  -2.3 -0.5  0.0 -0.4\n",
       "topic14  -0.8 -0.1   8.9   0.2 -0.8   6.4    0.1  -0.1   4.3 -0.0  0.1 -0.3\n",
       "topic15  -7.4 -0.5  -1.6   0.1 -1.4  -2.4   -0.9   0.5   1.0 -0.4  0.1 -0.1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 12\n",
    "deals = weights['! ;) :) half off free crazy deal only $ 80 %'.split()].round(3) * 100\n",
    "deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic0    -11.9\n",
       "topic1      7.6\n",
       "topic2     12.7\n",
       "topic3    -15.5\n",
       "topic4     38.5\n",
       "topic5    -33.8\n",
       "topic6      4.6\n",
       "topic7     -5.0\n",
       "topic8     40.5\n",
       "topic9     32.5\n",
       "topic10   -30.9\n",
       "topic11   -43.6\n",
       "topic12    22.4\n",
       "topic13    49.9\n",
       "topic14    17.9\n",
       "topic15   -13.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deals.T.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       ...,\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       ...,\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05],\n",
       "       [-2.56432277e-02, -5.84019981e-03, -2.28435274e-04, ...,\n",
       "        -5.48526215e-05, -5.48526215e-05, -5.48526215e-05]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_docs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>...</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.201</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  topic3  topic4  topic5  ...  topic10  topic11  topic12  topic13  topic14  topic15\n",
       "sms0    0.201   0.003   0.037   0.011  -0.019  -0.053  ...    0.007   -0.007    0.002   -0.036   -0.014    0.037\n",
       "sms1    0.404  -0.094  -0.078   0.051   0.100   0.047  ...   -0.004    0.036    0.043   -0.021    0.051   -0.042\n",
       "sms2!  -0.030  -0.048   0.090  -0.067   0.091  -0.043  ...    0.125    0.023    0.026   -0.020   -0.042    0.052\n",
       "sms3    0.329  -0.033  -0.035  -0.016   0.052   0.056  ...    0.022    0.023    0.073   -0.046    0.022   -0.070\n",
       "sms4    0.002   0.031   0.038   0.034  -0.075  -0.093  ...    0.028   -0.009    0.027    0.034   -0.083   -0.021\n",
       "sms5!  -0.016   0.059   0.014  -0.006   0.122  -0.040  ...    0.041    0.055   -0.037    0.075   -0.001    0.020\n",
       "\n",
       "[6 rows x 16 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=16, n_iter=100)\n",
    "svd_topic_vectors = svd.fit_transform(tfidf_docs.values)\n",
    "svd_topic_vectors = pd.DataFrame(svd_topic_vectors, columns=columns,index=index)\n",
    "svd_topic_vectors.round(3).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms0</th>\n",
       "      <th>sms1</th>\n",
       "      <th>sms2!</th>\n",
       "      <th>sms3</th>\n",
       "      <th>sms4</th>\n",
       "      <th>sms5!</th>\n",
       "      <th>sms6</th>\n",
       "      <th>sms7</th>\n",
       "      <th>sms8!</th>\n",
       "      <th>sms9!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms6</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms7</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms8!</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms9!</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sms0  sms1  sms2!  sms3  sms4  sms5!  sms6  sms7  sms8!  sms9!\n",
       "sms0    1.0   0.6   -0.1   0.6  -0.0   -0.3  -0.3  -0.1   -0.3   -0.3\n",
       "sms1    0.6   1.0   -0.2   0.8  -0.2    0.0  -0.2  -0.2   -0.1   -0.1\n",
       "sms2!  -0.1  -0.2    1.0  -0.2   0.1    0.4   0.0   0.3    0.5    0.4\n",
       "sms3    0.6   0.8   -0.2   1.0  -0.2   -0.3  -0.1  -0.3   -0.2   -0.1\n",
       "sms4   -0.0  -0.2    0.1  -0.2   1.0    0.2   0.0   0.1   -0.4   -0.2\n",
       "sms5!  -0.3   0.0    0.4  -0.3   0.2    1.0  -0.1   0.1    0.3    0.4\n",
       "sms6   -0.3  -0.2    0.0  -0.1   0.0   -0.1   1.0   0.1   -0.2   -0.2\n",
       "sms7   -0.1  -0.2    0.3  -0.3   0.1    0.1   0.1   1.0    0.1    0.4\n",
       "sms8!  -0.3  -0.1    0.5  -0.2  -0.4    0.3  -0.2   0.1    1.0    0.3\n",
       "sms9!  -0.3  -0.1    0.4  -0.1  -0.2    0.4  -0.2   0.4    0.3    1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "svd_topic_vectors = (svd_topic_vectors.T / np.linalg.norm(svd_topic_vectors, axis=1)).T\n",
    "svd_topic_vectors.iloc[:10].dot(svd_topic_vectors.iloc[:10].T).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.35"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_corpus_len = 0\n",
    "for document_text in sms.text:\n",
    "    total_corpus_len += len(casual_tokenize(document_text))\n",
    "mean_document_len = total_corpus_len / len(sms)\n",
    "round(mean_document_len, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.34794293983874"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(casual_tokenize(t)) for t in sms.text]) * 1. / len(sms.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=sms.text).toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.loc['sms0'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",            1\n",
       "..           1\n",
       "...          2\n",
       "amore        1\n",
       "available    1\n",
       "Name: sms0, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_docs.loc['sms0'][bow_docs.loc['sms0'] > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 9232)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia = LDiA(n_components=16, learning_method='batch')\n",
    "ldia = ldia.fit(bow_docs)\n",
    "ldia.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>...</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>184.03</td>\n",
       "      <td>15.00</td>\n",
       "      <td>72.22</td>\n",
       "      <td>394.95</td>\n",
       "      <td>45.48</td>\n",
       "      <td>36.14</td>\n",
       "      <td>...</td>\n",
       "      <td>37.42</td>\n",
       "      <td>44.18</td>\n",
       "      <td>64.40</td>\n",
       "      <td>297.29</td>\n",
       "      <td>41.16</td>\n",
       "      <td>11.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>0.68</td>\n",
       "      <td>4.22</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>152.35</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.42</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>62.72</td>\n",
       "      <td>12.27</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.07</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic0  topic1  topic2  topic3  topic4  topic5  ...  topic10  topic11  \\\n",
       "!  184.03   15.00   72.22  394.95   45.48   36.14  ...    37.42    44.18   \n",
       "\"    0.68    4.22    2.41    0.06  152.35    0.06  ...     8.42    11.42   \n",
       "#    0.06    0.06    0.06    0.06    0.06    2.07  ...     0.06     0.06   \n",
       "\n",
       "   topic12  topic13  topic14  topic15  \n",
       "!    64.40   297.29    41.16    11.70  \n",
       "\"     0.07    62.72    12.27     0.06  \n",
       "#     1.07     4.05     0.06     0.06  \n",
       "\n",
       "[3 rows x 16 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.width', 75)\n",
    "components = pd.DataFrame(ldia.components_.T, index=terms,columns=columns)\n",
    "components.round(2).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "!       394.952246\n",
       ".       218.049724\n",
       "to      119.533134\n",
       "u       118.857546\n",
       "call    111.948541\n",
       "£       107.358914\n",
       ",        96.954384\n",
       "*        90.314783\n",
       "your     90.215961\n",
       "is       75.750037\n",
       "Name: topic3, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components.topic3.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>...</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  topic3  topic4  topic5  ...  topic10  \\\n",
       "sms0     0.00    0.62    0.00    0.00    0.00    0.00  ...     0.00   \n",
       "sms1     0.01    0.01    0.01    0.01    0.01    0.01  ...     0.01   \n",
       "sms2!    0.00    0.00    0.00    0.00    0.00    0.00  ...     0.00   \n",
       "sms3     0.00    0.00    0.00    0.00    0.09    0.00  ...     0.00   \n",
       "sms4     0.39    0.00    0.33    0.00    0.00    0.00  ...     0.00   \n",
       "\n",
       "       topic11  topic12  topic13  topic14  topic15  \n",
       "sms0      0.00     0.00     0.00     0.00     0.00  \n",
       "sms1      0.12     0.01     0.01     0.01     0.01  \n",
       "sms2!     0.00     0.00     0.00     0.00     0.00  \n",
       "sms3      0.00     0.00     0.00     0.00     0.00  \n",
       "sms4      0.00     0.09     0.00     0.00     0.00  \n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldia16_topic_vectors = ldia.transform(bow_docs)\n",
    "ldia16_topic_vectors = pd.DataFrame(ldia16_topic_vectors,index=index, columns=columns)\n",
    "ldia16_topic_vectors.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "X_train, X_test, y_train, y_test =train_test_split(ldia16_topic_vectors, sms.spam, test_size=0.5,random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "sms['ldia16_spam'] = lda.predict(ldia16_topic_vectors)\n",
    "round(float(lda.score(X_test, y_test)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from itertools import product\\nall_pairs = [(word1, word2) for (word1, word2) in product(word_list,word_list) if not word1 == word2]'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from itertools import product\n",
    "all_pairs = [(word1, word2) for (word1, word2) in product(word_list,word_list) if not word1 == word2]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_docs,sms.spam.values, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "round(float(lda.score(X_train, y_train)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(float(lda.score(X_test, y_test)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 9232)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldia32 = LDiA(n_components=32, learning_method='batch')\n",
    "ldia32 = ldia32.fit(bow_docs)\n",
    "ldia32.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>...</th>\n",
       "      <th>topic26</th>\n",
       "      <th>topic27</th>\n",
       "      <th>topic28</th>\n",
       "      <th>topic29</th>\n",
       "      <th>topic30</th>\n",
       "      <th>topic31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  topic3  topic4  topic5  ...  topic26  \\\n",
       "sms0      0.0    0.00     0.0    0.06    0.14    0.00  ...      0.0   \n",
       "sms1      0.0    0.00     0.0    0.00    0.53    0.00  ...      0.0   \n",
       "sms2!     0.0    0.00     0.0    0.00    0.00    0.65  ...      0.0   \n",
       "sms3      0.0    0.11     0.0    0.00    0.39    0.00  ...      0.0   \n",
       "sms4      0.0    0.00     0.0    0.00    0.00    0.00  ...      0.0   \n",
       "\n",
       "       topic27  topic28  topic29  topic30  topic31  \n",
       "sms0      0.00      0.0     0.00      0.0      0.0  \n",
       "sms1      0.00      0.0     0.14      0.0      0.0  \n",
       "sms2!     0.00      0.0     0.00      0.0      0.0  \n",
       "sms3      0.00      0.0     0.00      0.0      0.0  \n",
       "sms4      0.47      0.0     0.00      0.0      0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldia32_topic_vectors = ldia32.transform(bow_docs)\n",
    "columns32 = ['topic{}'.format(i) for i in range(ldia32.n_components)]\n",
    "ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors, index=index,columns=columns32)\n",
    "ldia32_topic_vectors.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2418, 32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(ldia32_topic_vectors, sms.spam, test_size=0.5,random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "sms['ldia32_spam'] = lda.predict(ldia32_topic_vectors)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.933"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(float(lda.score(X_train, y_train)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(float(lda.score(X_test, y_test)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yule'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'braycurtis',\n",
    "'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard',\n",
    "'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
    "'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
    "'yule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(tfidf_docs, sms.spam)\n",
    "sms['lda_spaminess'] = lda.predict(tfidf_docs)\n",
    "((sms.spam - sms.lda_spaminess) ** 2.).sum() ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4837"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sms.spam == sms.lda_spaminess).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4837"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.76 (+/-0.03)'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lda = LDA(n_components=1)\n",
    "scores = cross_val_score(lda, tfidf_docs, sms.spam, cv=5)\n",
    "\"Accuracy: {:.2f} (+/-{:.2f})\".format(scores.mean(), scores.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\python\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=1, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_docs,sms.spam, test_size=0.33, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.765"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.score(X_test, y_test).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=1, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(pca_topic_vectors.values, sms.spam, test_size=0.3,random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.962"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.score(X_test, y_test).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.957 (+/-0.021)'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(n_components=1)\n",
    "scores = cross_val_score(lda, pca_topic_vectors, sms.spam, cv=10)\n",
    "\"Accuracy: {:.3f} (+/-{:.3f})\".format(scores.mean(), scores.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
